{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoe_4xUgvYfz"
   },
   "source": [
    "**Machine Learning**\n",
    "\n",
    "Machine learning is a subset of artificial intelligence focused on building systems that can learn from historical data, identify patterns, and make logical decisions with little to no human intervention. It is a data analysis method that automates the building of analytical models through using data that encompasses diverse forms of digital information including numbers, words, clicks and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq0bJscIvPZU"
   },
   "source": [
    "**Machine Learning Needs**\n",
    "\n",
    "There are a multitude of use cases that machine learning can be applied to in order to cut costs, mitigate risks, and improve overall quality of life including recommending products/services, detecting cybersecurity breaches, and enabling self-driving cars. With greater access to data and computation power, machine learning is becoming more ubiquitous every day and will soon be integrated into many facets of human life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiF-5LXAvlU3"
   },
   "source": [
    "**Machine Learning methods**\n",
    "\n",
    "**What is supervised Machine Learning**\n",
    "\n",
    "Supervised machine learning algorithms use labeled data as training data where the appropriate outputs to input data are known. The machine learning algorithm ingests a set of inputs and corresponding correct outputs. The algorithm compares its own predicted outputs with the correct outputs to calculate model accuracy and then optimizes model parameters to improve accuracy.\n",
    "\n",
    "Supervised machine learning relies on patterns to predict values on unlabeled data. It is most often used in automation, over large amounts of data records or in cases where there are too many data inputs for humans to process effectively. For example, the algorithm can pick up credit card transactions that are likely to be fraudulent or identify the insurance customer who will most probably file a claim.\n",
    "\n",
    "In supervised learning, we train the machine using the labeled dataset. It works on supervision where labeled data specifies that inputs are already labeled to the output. First, we train the machine with input data and the corresponding output. Later, we ask to predict the outcome using the test dataset.\n",
    "\n",
    "Supervised learning algorithms have interesting real-world applications: risk assessment, churn prediction, spam filtering, fraud detection, etc.\n",
    "\n",
    "Supervised learning algorithms can be further classified into two types.\n",
    "\n",
    "    Regression\n",
    "\n",
    "Regression shows the linear relationship between input (x) and output variable (y). The regression algorithms predict continuous output variables, such as weather prediction, house price prediction, market trends, etc.\n",
    "\n",
    "The mathematical equation for linear regression is; Y = a + bX. Here, X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept.\n",
    "\n",
    "    Classification\n",
    "\n",
    "Classification algorithms can solve classification problems where the output variable is categorical, such as churned or non-churned. The classification algorithms forecast the categories present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkFg1Csx6FFx"
   },
   "source": [
    "**Classification Algorithms**\n",
    "\n",
    "There are various types of classifiers algorithms. Some of them are :\n",
    "Linear Classifiers\n",
    "\n",
    "Linear models create a linear decision boundary between classes. They are simple and computationally efficient. Some of the linear classification models are as follows:\n",
    "\n",
    "    Logistic Regression\n",
    "    Support Vector Machines having kernel = ‘linear’\n",
    "    Single-layer Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3ldsMxS6YxW"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ms8rj1hd7N6B"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zxHqVssl7X0y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0  ...          17.33           184.60      2019.0            0.1622   \n",
      "1  ...          23.41           158.80      1956.0            0.1238   \n",
      "2  ...          25.53           152.50      1709.0            0.1444   \n",
      "3  ...          26.50            98.87       567.7            0.2098   \n",
      "4  ...          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "3             0.8663           0.6869                0.2575          0.6638   \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "3                  0.17300          NaN  \n",
      "4                  0.07678          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\annaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "C:\\annaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "C:\\annaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0            1        17.99         10.38          122.80     1001.0   \n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568          0         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% import dataset\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "print(data.head())\n",
    "data.drop(['Unnamed: 32',\"id\"], axis=1, inplace=True)\n",
    "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n",
    "y = data.diagnosis.values\n",
    "x_data = data.drop(['diagnosis'], axis=1)\n",
    "x = (x_data -np.min(x_data))/(np.max(x_data)-np.min(x_data))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    357\n",
       "1    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUbYhYEs9TE1",
    "outputId": "64f1a6bd-bdc2-4131-9963-0e3bfec1b31a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (30, 341)\n",
      "x test:  (30, 228)\n",
      "y train:  (341,)\n",
      "y test:  (228,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42,stratify=y)\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "print(\"x train: \",x_train.shape)\n",
    "print(\"x test: \",x_test.shape)\n",
    "print(\"y train: \",y_train.shape)\n",
    "print(\"y test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y-5TpUpXCIAr"
   },
   "outputs": [],
   "source": [
    "# %%initialize\n",
    "# lets initialize parameters\n",
    "# So what we need is dimension 4096 that is number of pixels as a parameter for our initialize method(def)\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kZdB3mQfEmjh"
   },
   "outputs": [],
   "source": [
    "#%% sigmoid\n",
    "# calculation of z\n",
    "#z = np.dot(w.T,x_train)+b\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head\n",
    "#y_head = sigmoid(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2lsqXwp7Eo7v"
   },
   "outputs": [],
   "source": [
    "#%% forward and backward\n",
    "# In backward propagation we will use y_head that found in forward progation\n",
    "# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\n",
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jlzvj3fAEuVu"
   },
   "outputs": [],
   "source": [
    "#%%# Updating(learning) parameters\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "r5qThtL9Euu1"
   },
   "outputs": [],
   "source": [
    "#%%  # prediction\n",
    "def predict(w,b,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>65</th>\n",
       "      <th>470</th>\n",
       "      <th>486</th>\n",
       "      <th>389</th>\n",
       "      <th>473</th>\n",
       "      <th>67</th>\n",
       "      <th>227</th>\n",
       "      <th>390</th>\n",
       "      <th>327</th>\n",
       "      <th>261</th>\n",
       "      <th>...</th>\n",
       "      <th>400</th>\n",
       "      <th>549</th>\n",
       "      <th>92</th>\n",
       "      <th>333</th>\n",
       "      <th>324</th>\n",
       "      <th>192</th>\n",
       "      <th>37</th>\n",
       "      <th>415</th>\n",
       "      <th>458</th>\n",
       "      <th>476</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>radius_mean</th>\n",
       "      <td>0.369114</td>\n",
       "      <td>0.127124</td>\n",
       "      <td>0.362488</td>\n",
       "      <td>0.594870</td>\n",
       "      <td>0.250319</td>\n",
       "      <td>0.204884</td>\n",
       "      <td>0.379526</td>\n",
       "      <td>0.155190</td>\n",
       "      <td>0.238961</td>\n",
       "      <td>0.490747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517251</td>\n",
       "      <td>0.181693</td>\n",
       "      <td>0.297648</td>\n",
       "      <td>0.202045</td>\n",
       "      <td>0.247006</td>\n",
       "      <td>0.129632</td>\n",
       "      <td>0.286289</td>\n",
       "      <td>0.232335</td>\n",
       "      <td>0.284869</td>\n",
       "      <td>0.341663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_mean</th>\n",
       "      <td>0.481231</td>\n",
       "      <td>0.296923</td>\n",
       "      <td>0.241461</td>\n",
       "      <td>0.456544</td>\n",
       "      <td>0.685154</td>\n",
       "      <td>0.315522</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.277984</td>\n",
       "      <td>0.451471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382482</td>\n",
       "      <td>0.490362</td>\n",
       "      <td>0.170781</td>\n",
       "      <td>0.171458</td>\n",
       "      <td>0.185999</td>\n",
       "      <td>0.287792</td>\n",
       "      <td>0.294555</td>\n",
       "      <td>0.387555</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>0.365911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_mean</th>\n",
       "      <td>0.370465</td>\n",
       "      <td>0.122314</td>\n",
       "      <td>0.348421</td>\n",
       "      <td>0.588142</td>\n",
       "      <td>0.232396</td>\n",
       "      <td>0.193560</td>\n",
       "      <td>0.370811</td>\n",
       "      <td>0.151752</td>\n",
       "      <td>0.223205</td>\n",
       "      <td>0.464446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557045</td>\n",
       "      <td>0.173450</td>\n",
       "      <td>0.282980</td>\n",
       "      <td>0.190657</td>\n",
       "      <td>0.236473</td>\n",
       "      <td>0.117062</td>\n",
       "      <td>0.268261</td>\n",
       "      <td>0.225278</td>\n",
       "      <td>0.268261</td>\n",
       "      <td>0.335982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_mean</th>\n",
       "      <td>0.222609</td>\n",
       "      <td>0.061760</td>\n",
       "      <td>0.221633</td>\n",
       "      <td>0.437116</td>\n",
       "      <td>0.136543</td>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.229480</td>\n",
       "      <td>0.075546</td>\n",
       "      <td>0.128314</td>\n",
       "      <td>0.334931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360764</td>\n",
       "      <td>0.092513</td>\n",
       "      <td>0.173150</td>\n",
       "      <td>0.104560</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.061336</td>\n",
       "      <td>0.161315</td>\n",
       "      <td>0.123139</td>\n",
       "      <td>0.159788</td>\n",
       "      <td>0.201442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_mean</th>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.332491</td>\n",
       "      <td>0.304956</td>\n",
       "      <td>0.436671</td>\n",
       "      <td>0.219915</td>\n",
       "      <td>0.259637</td>\n",
       "      <td>0.280581</td>\n",
       "      <td>0.427282</td>\n",
       "      <td>0.218471</td>\n",
       "      <td>0.306852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635280</td>\n",
       "      <td>0.264422</td>\n",
       "      <td>0.188860</td>\n",
       "      <td>0.274713</td>\n",
       "      <td>0.307845</td>\n",
       "      <td>0.152298</td>\n",
       "      <td>0.335831</td>\n",
       "      <td>0.407150</td>\n",
       "      <td>0.280401</td>\n",
       "      <td>0.331137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_mean</th>\n",
       "      <td>0.394209</td>\n",
       "      <td>0.132507</td>\n",
       "      <td>0.146003</td>\n",
       "      <td>0.344825</td>\n",
       "      <td>0.044783</td>\n",
       "      <td>0.084749</td>\n",
       "      <td>0.276731</td>\n",
       "      <td>0.171891</td>\n",
       "      <td>0.059935</td>\n",
       "      <td>0.133489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730691</td>\n",
       "      <td>0.143059</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.077296</td>\n",
       "      <td>0.141310</td>\n",
       "      <td>0.012453</td>\n",
       "      <td>0.056070</td>\n",
       "      <td>0.189620</td>\n",
       "      <td>0.096160</td>\n",
       "      <td>0.280412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_mean</th>\n",
       "      <td>0.296860</td>\n",
       "      <td>0.069072</td>\n",
       "      <td>0.121649</td>\n",
       "      <td>0.434864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086903</td>\n",
       "      <td>0.152413</td>\n",
       "      <td>0.045056</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.067737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747188</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>0.076406</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.046720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060028</td>\n",
       "      <td>0.059864</td>\n",
       "      <td>0.028257</td>\n",
       "      <td>0.118627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_mean</th>\n",
       "      <td>0.448757</td>\n",
       "      <td>0.075249</td>\n",
       "      <td>0.138718</td>\n",
       "      <td>0.507455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110835</td>\n",
       "      <td>0.187873</td>\n",
       "      <td>0.097813</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595427</td>\n",
       "      <td>0.040557</td>\n",
       "      <td>0.131610</td>\n",
       "      <td>0.014617</td>\n",
       "      <td>0.084095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145278</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.087575</td>\n",
       "      <td>0.151988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_mean</th>\n",
       "      <td>0.451010</td>\n",
       "      <td>0.594949</td>\n",
       "      <td>0.176263</td>\n",
       "      <td>0.469192</td>\n",
       "      <td>0.323737</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.414646</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.162626</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531818</td>\n",
       "      <td>0.462626</td>\n",
       "      <td>0.164646</td>\n",
       "      <td>0.360101</td>\n",
       "      <td>0.291919</td>\n",
       "      <td>0.299495</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.484343</td>\n",
       "      <td>0.306566</td>\n",
       "      <td>0.225253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.298441</td>\n",
       "      <td>0.075611</td>\n",
       "      <td>0.187026</td>\n",
       "      <td>0.203033</td>\n",
       "      <td>0.141323</td>\n",
       "      <td>0.191870</td>\n",
       "      <td>0.331297</td>\n",
       "      <td>0.226201</td>\n",
       "      <td>0.065501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446293</td>\n",
       "      <td>0.280539</td>\n",
       "      <td>0.067818</td>\n",
       "      <td>0.228517</td>\n",
       "      <td>0.238627</td>\n",
       "      <td>0.305602</td>\n",
       "      <td>0.182603</td>\n",
       "      <td>0.272536</td>\n",
       "      <td>0.095409</td>\n",
       "      <td>0.213353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_se</th>\n",
       "      <td>0.089154</td>\n",
       "      <td>0.096361</td>\n",
       "      <td>0.039435</td>\n",
       "      <td>0.180771</td>\n",
       "      <td>0.120949</td>\n",
       "      <td>0.058374</td>\n",
       "      <td>0.043563</td>\n",
       "      <td>0.028825</td>\n",
       "      <td>0.044179</td>\n",
       "      <td>0.104726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105559</td>\n",
       "      <td>0.147782</td>\n",
       "      <td>0.106536</td>\n",
       "      <td>0.037262</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>0.087778</td>\n",
       "      <td>0.026218</td>\n",
       "      <td>0.059098</td>\n",
       "      <td>0.054536</td>\n",
       "      <td>0.085569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_se</th>\n",
       "      <td>0.203501</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.142725</td>\n",
       "      <td>0.547162</td>\n",
       "      <td>0.726397</td>\n",
       "      <td>0.128779</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>0.121442</td>\n",
       "      <td>0.211457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091606</td>\n",
       "      <td>0.344280</td>\n",
       "      <td>0.175212</td>\n",
       "      <td>0.140537</td>\n",
       "      <td>0.098811</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437986</td>\n",
       "      <td>0.186262</td>\n",
       "      <td>0.192671</td>\n",
       "      <td>0.145377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_se</th>\n",
       "      <td>0.079772</td>\n",
       "      <td>0.085379</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.217971</td>\n",
       "      <td>0.100221</td>\n",
       "      <td>0.050605</td>\n",
       "      <td>0.071573</td>\n",
       "      <td>0.027847</td>\n",
       "      <td>0.033407</td>\n",
       "      <td>0.085756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111483</td>\n",
       "      <td>0.132262</td>\n",
       "      <td>0.091599</td>\n",
       "      <td>0.036376</td>\n",
       "      <td>0.056637</td>\n",
       "      <td>0.069406</td>\n",
       "      <td>0.019460</td>\n",
       "      <td>0.055270</td>\n",
       "      <td>0.042407</td>\n",
       "      <td>0.093860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_se</th>\n",
       "      <td>0.053116</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>0.024613</td>\n",
       "      <td>0.118226</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0.021195</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>0.018991</td>\n",
       "      <td>0.070243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064827</td>\n",
       "      <td>0.048932</td>\n",
       "      <td>0.055189</td>\n",
       "      <td>0.015443</td>\n",
       "      <td>0.022802</td>\n",
       "      <td>0.027807</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>0.023773</td>\n",
       "      <td>0.026873</td>\n",
       "      <td>0.045215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_se</th>\n",
       "      <td>0.169630</td>\n",
       "      <td>0.196757</td>\n",
       "      <td>0.061937</td>\n",
       "      <td>0.323860</td>\n",
       "      <td>0.191250</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.081789</td>\n",
       "      <td>0.134922</td>\n",
       "      <td>0.102526</td>\n",
       "      <td>0.136418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185131</td>\n",
       "      <td>0.222660</td>\n",
       "      <td>0.094095</td>\n",
       "      <td>0.132712</td>\n",
       "      <td>0.125438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089710</td>\n",
       "      <td>0.278138</td>\n",
       "      <td>0.147568</td>\n",
       "      <td>0.081382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_se</th>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.132469</td>\n",
       "      <td>0.087707</td>\n",
       "      <td>0.290789</td>\n",
       "      <td>0.044995</td>\n",
       "      <td>0.052303</td>\n",
       "      <td>0.223946</td>\n",
       "      <td>0.085604</td>\n",
       "      <td>0.034811</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262325</td>\n",
       "      <td>0.123532</td>\n",
       "      <td>0.061045</td>\n",
       "      <td>0.036591</td>\n",
       "      <td>0.089585</td>\n",
       "      <td>0.033677</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>0.212380</td>\n",
       "      <td>0.050485</td>\n",
       "      <td>0.230030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_se</th>\n",
       "      <td>0.058460</td>\n",
       "      <td>0.068535</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.188611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052096</td>\n",
       "      <td>0.092020</td>\n",
       "      <td>0.021455</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.031465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155682</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>0.034293</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.026540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033914</td>\n",
       "      <td>0.041162</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.071237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_se</th>\n",
       "      <td>0.224285</td>\n",
       "      <td>0.187213</td>\n",
       "      <td>0.116386</td>\n",
       "      <td>0.651828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>0.218791</td>\n",
       "      <td>0.131256</td>\n",
       "      <td>0.074844</td>\n",
       "      <td>0.145312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199091</td>\n",
       "      <td>0.112086</td>\n",
       "      <td>0.204963</td>\n",
       "      <td>0.055711</td>\n",
       "      <td>0.097405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220496</td>\n",
       "      <td>0.175715</td>\n",
       "      <td>0.120023</td>\n",
       "      <td>0.255730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_se</th>\n",
       "      <td>0.156442</td>\n",
       "      <td>0.164884</td>\n",
       "      <td>0.065543</td>\n",
       "      <td>0.278578</td>\n",
       "      <td>0.331063</td>\n",
       "      <td>0.196263</td>\n",
       "      <td>0.084820</td>\n",
       "      <td>0.161789</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.087634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112962</td>\n",
       "      <td>0.236084</td>\n",
       "      <td>0.039511</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.076659</td>\n",
       "      <td>0.423651</td>\n",
       "      <td>0.264929</td>\n",
       "      <td>0.206816</td>\n",
       "      <td>0.060055</td>\n",
       "      <td>0.115636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <td>0.080469</td>\n",
       "      <td>0.104273</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.184666</td>\n",
       "      <td>0.077429</td>\n",
       "      <td>0.043227</td>\n",
       "      <td>0.079778</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.029718</td>\n",
       "      <td>0.023603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145247</td>\n",
       "      <td>0.071936</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.040428</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>0.030478</td>\n",
       "      <td>0.047580</td>\n",
       "      <td>0.021392</td>\n",
       "      <td>0.063886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_worst</th>\n",
       "      <td>0.333689</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>0.303451</td>\n",
       "      <td>0.458556</td>\n",
       "      <td>0.196371</td>\n",
       "      <td>0.156528</td>\n",
       "      <td>0.301672</td>\n",
       "      <td>0.122732</td>\n",
       "      <td>0.182853</td>\n",
       "      <td>0.424048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457844</td>\n",
       "      <td>0.181430</td>\n",
       "      <td>0.299893</td>\n",
       "      <td>0.171825</td>\n",
       "      <td>0.207044</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.191035</td>\n",
       "      <td>0.182142</td>\n",
       "      <td>0.228033</td>\n",
       "      <td>0.303095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_worst</th>\n",
       "      <td>0.569563</td>\n",
       "      <td>0.362473</td>\n",
       "      <td>0.357676</td>\n",
       "      <td>0.490938</td>\n",
       "      <td>0.693763</td>\n",
       "      <td>0.315032</td>\n",
       "      <td>0.194296</td>\n",
       "      <td>0.096748</td>\n",
       "      <td>0.272655</td>\n",
       "      <td>0.518390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420043</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.275320</td>\n",
       "      <td>0.267591</td>\n",
       "      <td>0.249467</td>\n",
       "      <td>0.234808</td>\n",
       "      <td>0.287580</td>\n",
       "      <td>0.404851</td>\n",
       "      <td>0.529318</td>\n",
       "      <td>0.406183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_worst</th>\n",
       "      <td>0.319687</td>\n",
       "      <td>0.101947</td>\n",
       "      <td>0.276856</td>\n",
       "      <td>0.456148</td>\n",
       "      <td>0.172668</td>\n",
       "      <td>0.137407</td>\n",
       "      <td>0.317695</td>\n",
       "      <td>0.113651</td>\n",
       "      <td>0.161014</td>\n",
       "      <td>0.387420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493999</td>\n",
       "      <td>0.166791</td>\n",
       "      <td>0.269386</td>\n",
       "      <td>0.157727</td>\n",
       "      <td>0.202699</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>0.169580</td>\n",
       "      <td>0.172718</td>\n",
       "      <td>0.202450</td>\n",
       "      <td>0.307236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_worst</th>\n",
       "      <td>0.181847</td>\n",
       "      <td>0.049155</td>\n",
       "      <td>0.158720</td>\n",
       "      <td>0.277182</td>\n",
       "      <td>0.091845</td>\n",
       "      <td>0.069185</td>\n",
       "      <td>0.153116</td>\n",
       "      <td>0.051440</td>\n",
       "      <td>0.083120</td>\n",
       "      <td>0.253834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274971</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.158622</td>\n",
       "      <td>0.075575</td>\n",
       "      <td>0.097793</td>\n",
       "      <td>0.029149</td>\n",
       "      <td>0.088650</td>\n",
       "      <td>0.082997</td>\n",
       "      <td>0.108951</td>\n",
       "      <td>0.158106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_worst</th>\n",
       "      <td>0.618305</td>\n",
       "      <td>0.344912</td>\n",
       "      <td>0.284158</td>\n",
       "      <td>0.356138</td>\n",
       "      <td>0.152216</td>\n",
       "      <td>0.381893</td>\n",
       "      <td>0.280195</td>\n",
       "      <td>0.416892</td>\n",
       "      <td>0.198970</td>\n",
       "      <td>0.348874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766889</td>\n",
       "      <td>0.325101</td>\n",
       "      <td>0.194347</td>\n",
       "      <td>0.300007</td>\n",
       "      <td>0.359440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170640</td>\n",
       "      <td>0.471703</td>\n",
       "      <td>0.334346</td>\n",
       "      <td>0.291422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_worst</th>\n",
       "      <td>0.304945</td>\n",
       "      <td>0.123129</td>\n",
       "      <td>0.174356</td>\n",
       "      <td>0.207731</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.062277</td>\n",
       "      <td>0.325416</td>\n",
       "      <td>0.133607</td>\n",
       "      <td>0.045221</td>\n",
       "      <td>0.117696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547593</td>\n",
       "      <td>0.131958</td>\n",
       "      <td>0.093634</td>\n",
       "      <td>0.068545</td>\n",
       "      <td>0.160579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018337</td>\n",
       "      <td>0.185707</td>\n",
       "      <td>0.079567</td>\n",
       "      <td>0.306206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_worst</th>\n",
       "      <td>0.241534</td>\n",
       "      <td>0.101997</td>\n",
       "      <td>0.194649</td>\n",
       "      <td>0.305831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115335</td>\n",
       "      <td>0.271725</td>\n",
       "      <td>0.068810</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>0.096725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721565</td>\n",
       "      <td>0.049473</td>\n",
       "      <td>0.107827</td>\n",
       "      <td>0.004407</td>\n",
       "      <td>0.093211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038602</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.035639</td>\n",
       "      <td>0.200639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_worst</th>\n",
       "      <td>0.554639</td>\n",
       "      <td>0.225430</td>\n",
       "      <td>0.269003</td>\n",
       "      <td>0.627148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239210</td>\n",
       "      <td>0.473883</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.096082</td>\n",
       "      <td>0.282990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.112165</td>\n",
       "      <td>0.343986</td>\n",
       "      <td>0.057285</td>\n",
       "      <td>0.190928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172268</td>\n",
       "      <td>0.283952</td>\n",
       "      <td>0.203471</td>\n",
       "      <td>0.460137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_worst</th>\n",
       "      <td>0.346146</td>\n",
       "      <td>0.317169</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.199290</td>\n",
       "      <td>0.166371</td>\n",
       "      <td>0.164597</td>\n",
       "      <td>0.273802</td>\n",
       "      <td>0.270451</td>\n",
       "      <td>0.119456</td>\n",
       "      <td>0.174847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331165</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.246403</td>\n",
       "      <td>0.216046</td>\n",
       "      <td>0.067810</td>\n",
       "      <td>0.083185</td>\n",
       "      <td>0.297654</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.191011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <td>0.223468</td>\n",
       "      <td>0.198085</td>\n",
       "      <td>0.071625</td>\n",
       "      <td>0.137610</td>\n",
       "      <td>0.081267</td>\n",
       "      <td>0.074577</td>\n",
       "      <td>0.187459</td>\n",
       "      <td>0.145481</td>\n",
       "      <td>0.100551</td>\n",
       "      <td>0.066312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424767</td>\n",
       "      <td>0.139184</td>\n",
       "      <td>0.046045</td>\n",
       "      <td>0.125541</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.069198</td>\n",
       "      <td>0.043618</td>\n",
       "      <td>0.121147</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>0.154401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 341 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              65        470       486       389       473  \\\n",
       "radius_mean              0.369114  0.127124  0.362488  0.594870  0.250319   \n",
       "texture_mean             0.481231  0.296923  0.241461  0.456544  0.685154   \n",
       "perimeter_mean           0.370465  0.122314  0.348421  0.588142  0.232396   \n",
       "area_mean                0.222609  0.061760  0.221633  0.437116  0.136543   \n",
       "smoothness_mean          0.582920  0.332491  0.304956  0.436671  0.219915   \n",
       "compactness_mean         0.394209  0.132507  0.146003  0.344825  0.044783   \n",
       "concavity_mean           0.296860  0.069072  0.121649  0.434864  0.000000   \n",
       "concave points_mean      0.448757  0.075249  0.138718  0.507455  0.000000   \n",
       "symmetry_mean            0.451010  0.594949  0.176263  0.469192  0.323737   \n",
       "fractal_dimension_mean   0.349200  0.298441  0.075611  0.187026  0.203033   \n",
       "radius_se                0.089154  0.096361  0.039435  0.180771  0.120949   \n",
       "texture_se               0.203501  0.218750  0.142725  0.547162  0.726397   \n",
       "perimeter_se             0.079772  0.085379  0.033643  0.217971  0.100221   \n",
       "area_se                  0.053116  0.029750  0.024613  0.118226  0.052910   \n",
       "smoothness_se            0.169630  0.196757  0.061937  0.323860  0.191250   \n",
       "compactness_se           0.156578  0.132469  0.087707  0.290789  0.044995   \n",
       "concavity_se             0.058460  0.068535  0.045455  0.188611  0.000000   \n",
       "concave points_se        0.224285  0.187213  0.116386  0.651828  0.000000   \n",
       "symmetry_se              0.156442  0.164884  0.065543  0.278578  0.331063   \n",
       "fractal_dimension_se     0.080469  0.104273  0.011200  0.184666  0.077429   \n",
       "radius_worst             0.333689  0.114194  0.303451  0.458556  0.196371   \n",
       "texture_worst            0.569563  0.362473  0.357676  0.490938  0.693763   \n",
       "perimeter_worst          0.319687  0.101947  0.276856  0.456148  0.172668   \n",
       "area_worst               0.181847  0.049155  0.158720  0.277182  0.091845   \n",
       "smoothness_worst         0.618305  0.344912  0.284158  0.356138  0.152216   \n",
       "compactness_worst        0.304945  0.123129  0.174356  0.207731  0.024100   \n",
       "concavity_worst          0.241534  0.101997  0.194649  0.305831  0.000000   \n",
       "concave points_worst     0.554639  0.225430  0.269003  0.627148  0.000000   \n",
       "symmetry_worst           0.346146  0.317169  0.175439  0.199290  0.166371   \n",
       "fractal_dimension_worst  0.223468  0.198085  0.071625  0.137610  0.081267   \n",
       "\n",
       "                              67        227       390       327       261  \\\n",
       "radius_mean              0.204884  0.379526  0.155190  0.238961  0.490747   \n",
       "texture_mean             0.315522  0.196145  0.084883  0.277984  0.451471   \n",
       "perimeter_mean           0.193560  0.370811  0.151752  0.223205  0.464446   \n",
       "area_mean                0.106299  0.229480  0.075546  0.128314  0.334931   \n",
       "smoothness_mean          0.259637  0.280581  0.427282  0.218471  0.306852   \n",
       "compactness_mean         0.084749  0.276731  0.171891  0.059935  0.133489   \n",
       "concavity_mean           0.086903  0.152413  0.045056  0.003622  0.067737   \n",
       "concave points_mean      0.110835  0.187873  0.097813  0.027793  0.141004   \n",
       "symmetry_mean            0.230303  0.414646  0.373737  0.162626  0.254545   \n",
       "fractal_dimension_mean   0.141323  0.191870  0.331297  0.226201  0.065501   \n",
       "radius_se                0.058374  0.043563  0.028825  0.044179  0.104726   \n",
       "texture_se               0.128779  0.030145  0.041438  0.121442  0.211457   \n",
       "perimeter_se             0.050605  0.071573  0.027847  0.033407  0.085756   \n",
       "area_se                  0.021195  0.024427  0.009485  0.018991  0.070243   \n",
       "smoothness_se            0.257300  0.081789  0.134922  0.102526  0.136418   \n",
       "compactness_se           0.052303  0.223946  0.085604  0.034811  0.066152   \n",
       "concavity_se             0.052096  0.092020  0.021455  0.002990  0.031465   \n",
       "concave points_se        0.169824  0.218791  0.131256  0.074844  0.145312   \n",
       "symmetry_se              0.196263  0.084820  0.161789  0.095373  0.087634   \n",
       "fractal_dimension_se     0.043227  0.079778  0.051000  0.029718  0.023603   \n",
       "radius_worst             0.156528  0.301672  0.122732  0.182853  0.424048   \n",
       "texture_worst            0.315032  0.194296  0.096748  0.272655  0.518390   \n",
       "perimeter_worst          0.137407  0.317695  0.113651  0.161014  0.387420   \n",
       "area_worst               0.069185  0.153116  0.051440  0.083120  0.253834   \n",
       "smoothness_worst         0.381893  0.280195  0.416892  0.198970  0.348874   \n",
       "compactness_worst        0.062277  0.325416  0.133607  0.045221  0.117696   \n",
       "concavity_worst          0.115335  0.271725  0.068810  0.006176  0.096725   \n",
       "concave points_worst     0.239210  0.473883  0.230103  0.096082  0.282990   \n",
       "symmetry_worst           0.164597  0.273802  0.270451  0.119456  0.174847   \n",
       "fractal_dimension_worst  0.074577  0.187459  0.145481  0.100551  0.066312   \n",
       "\n",
       "                         ...       400       549       92        333  \\\n",
       "radius_mean              ...  0.517251  0.181693  0.297648  0.202045   \n",
       "texture_mean             ...  0.382482  0.490362  0.170781  0.171458   \n",
       "perimeter_mean           ...  0.557045  0.173450  0.282980  0.190657   \n",
       "area_mean                ...  0.360764  0.092513  0.173150  0.104560   \n",
       "smoothness_mean          ...  0.635280  0.264422  0.188860  0.274713   \n",
       "compactness_mean         ...  0.730691  0.143059  0.095608  0.077296   \n",
       "concavity_mean           ...  0.747188  0.036270  0.076406  0.002281   \n",
       "concave points_mean      ...  0.595427  0.040557  0.131610  0.014617   \n",
       "symmetry_mean            ...  0.531818  0.462626  0.164646  0.360101   \n",
       "fractal_dimension_mean   ...  0.446293  0.280539  0.067818  0.228517   \n",
       "radius_se                ...  0.105559  0.147782  0.106536  0.037262   \n",
       "texture_se               ...  0.091606  0.344280  0.175212  0.140537   \n",
       "perimeter_se             ...  0.111483  0.132262  0.091599  0.036376   \n",
       "area_se                  ...  0.064827  0.048932  0.055189  0.015443   \n",
       "smoothness_se            ...  0.185131  0.222660  0.094095  0.132712   \n",
       "compactness_se           ...  0.262325  0.123532  0.061045  0.036591   \n",
       "concavity_se             ...  0.155682  0.032247  0.034293  0.002459   \n",
       "concave points_se        ...  0.199091  0.112086  0.204963  0.055711   \n",
       "symmetry_se              ...  0.112962  0.236084  0.039511  0.128300   \n",
       "fractal_dimension_se     ...  0.145247  0.071936  0.018663  0.039219   \n",
       "radius_worst             ...  0.457844  0.181430  0.299893  0.171825   \n",
       "texture_worst            ...  0.420043  0.517857  0.275320  0.267591   \n",
       "perimeter_worst          ...  0.493999  0.166791  0.269386  0.157727   \n",
       "area_worst               ...  0.274971  0.078746  0.158622  0.075575   \n",
       "smoothness_worst         ...  0.766889  0.325101  0.194347  0.300007   \n",
       "compactness_worst        ...  0.547593  0.131958  0.093634  0.068545   \n",
       "concavity_worst          ...  0.721565  0.049473  0.107827  0.004407   \n",
       "concave points_worst     ...  0.674914  0.112165  0.343986  0.057285   \n",
       "symmetry_worst           ...  0.331165  0.294500  0.091070  0.246403   \n",
       "fractal_dimension_worst  ...  0.424767  0.139184  0.046045  0.125541   \n",
       "\n",
       "                              324       192       37        415       458  \\\n",
       "radius_mean              0.247006  0.129632  0.286289  0.232335  0.284869   \n",
       "texture_mean             0.185999  0.287792  0.294555  0.387555  0.521474   \n",
       "perimeter_mean           0.236473  0.117062  0.268261  0.225278  0.268261   \n",
       "area_mean                0.133362  0.061336  0.161315  0.123139  0.159788   \n",
       "smoothness_mean          0.307845  0.152298  0.335831  0.407150  0.280401   \n",
       "compactness_mean         0.141310  0.012453  0.056070  0.189620  0.096160   \n",
       "concavity_mean           0.046720  0.000000  0.060028  0.059864  0.028257   \n",
       "concave points_mean      0.084095  0.000000  0.145278  0.108300  0.087575   \n",
       "symmetry_mean            0.291919  0.299495  0.205556  0.484343  0.306566   \n",
       "fractal_dimension_mean   0.238627  0.305602  0.182603  0.272536  0.095409   \n",
       "radius_se                0.052870  0.087778  0.026218  0.059098  0.054536   \n",
       "texture_se               0.098811  1.000000  0.437986  0.186262  0.192671   \n",
       "perimeter_se             0.056637  0.069406  0.019460  0.055270  0.042407   \n",
       "area_se                  0.022802  0.027807  0.013743  0.023773  0.026873   \n",
       "smoothness_se            0.125438  0.000000  0.089710  0.278138  0.147568   \n",
       "compactness_se           0.089585  0.033677  0.019880  0.212380  0.050485   \n",
       "concavity_se             0.026540  0.000000  0.033914  0.041162  0.014346   \n",
       "concave points_se        0.097405  0.000000  0.220496  0.175715  0.120023   \n",
       "symmetry_se              0.076659  0.423651  0.264929  0.206816  0.060055   \n",
       "fractal_dimension_se     0.040428  0.027404  0.030478  0.047580  0.021392   \n",
       "radius_worst             0.207044  0.072501  0.191035  0.182142  0.228033   \n",
       "texture_worst            0.249467  0.234808  0.287580  0.404851  0.529318   \n",
       "perimeter_worst          0.202699  0.058967  0.169580  0.172718  0.202450   \n",
       "area_worst               0.097793  0.029149  0.088650  0.082997  0.108951   \n",
       "smoothness_worst         0.359440  0.000000  0.170640  0.471703  0.334346   \n",
       "compactness_worst        0.160579  0.000000  0.018337  0.185707  0.079567   \n",
       "concavity_worst          0.093211  0.000000  0.038602  0.092971  0.035639   \n",
       "concave points_worst     0.190928  0.000000  0.172268  0.283952  0.203471   \n",
       "symmetry_worst           0.216046  0.067810  0.083185  0.297654  0.146067   \n",
       "fractal_dimension_worst  0.161157  0.069198  0.043618  0.121147  0.051620   \n",
       "\n",
       "                              476  \n",
       "radius_mean              0.341663  \n",
       "texture_mean             0.365911  \n",
       "perimeter_mean           0.335982  \n",
       "area_mean                0.201442  \n",
       "smoothness_mean          0.331137  \n",
       "compactness_mean         0.280412  \n",
       "concavity_mean           0.118627  \n",
       "concave points_mean      0.151988  \n",
       "symmetry_mean            0.225253  \n",
       "fractal_dimension_mean   0.213353  \n",
       "radius_se                0.085569  \n",
       "texture_se               0.145377  \n",
       "perimeter_se             0.093860  \n",
       "area_se                  0.045215  \n",
       "smoothness_se            0.081382  \n",
       "compactness_se           0.230030  \n",
       "concavity_se             0.071237  \n",
       "concave points_se        0.255730  \n",
       "symmetry_se              0.115636  \n",
       "fractal_dimension_se     0.063886  \n",
       "radius_worst             0.303095  \n",
       "texture_worst            0.406183  \n",
       "perimeter_worst          0.307236  \n",
       "area_worst               0.158106  \n",
       "smoothness_worst         0.291422  \n",
       "compactness_worst        0.306206  \n",
       "concavity_worst          0.200639  \n",
       "concave points_worst     0.460137  \n",
       "symmetry_worst           0.191011  \n",
       "fractal_dimension_worst  0.154401  \n",
       "\n",
       "[30 rows x 341 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "id": "CU_EOfneEwst",
    "outputId": "6c9484ab-9c25-4a0c-9676-e6509c9c5529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]]\n",
      "Cost after iteration 0: 0.693068\n",
      "Cost after iteration 10: 0.663243\n",
      "Cost after iteration 20: 0.638950\n",
      "Cost after iteration 30: 0.617133\n",
      "Cost after iteration 40: 0.597080\n",
      "Cost after iteration 50: 0.578536\n",
      "Cost after iteration 60: 0.561348\n",
      "Cost after iteration 70: 0.545389\n",
      "Cost after iteration 80: 0.530546\n",
      "Cost after iteration 90: 0.516718\n",
      "Cost after iteration 100: 0.503811\n",
      "Cost after iteration 110: 0.491744\n",
      "Cost after iteration 120: 0.480441\n",
      "Cost after iteration 130: 0.469836\n",
      "Cost after iteration 140: 0.459868\n",
      "Cost after iteration 150: 0.450485\n",
      "Cost after iteration 160: 0.441636\n",
      "Cost after iteration 170: 0.433279\n",
      "Cost after iteration 180: 0.425373\n",
      "Cost after iteration 190: 0.417884\n",
      "Cost after iteration 200: 0.410780\n",
      "Cost after iteration 210: 0.404032\n",
      "Cost after iteration 220: 0.397613\n",
      "Cost after iteration 230: 0.391499\n",
      "Cost after iteration 240: 0.385670\n",
      "Cost after iteration 250: 0.380105\n",
      "Cost after iteration 260: 0.374787\n",
      "Cost after iteration 270: 0.369699\n",
      "Cost after iteration 280: 0.364826\n",
      "Cost after iteration 290: 0.360154\n",
      "Cost after iteration 300: 0.355672\n",
      "Cost after iteration 310: 0.351366\n",
      "Cost after iteration 320: 0.347228\n",
      "Cost after iteration 330: 0.343246\n",
      "Cost after iteration 340: 0.339412\n",
      "Cost after iteration 350: 0.335717\n",
      "Cost after iteration 360: 0.332154\n",
      "Cost after iteration 370: 0.328715\n",
      "Cost after iteration 380: 0.325394\n",
      "Cost after iteration 390: 0.322184\n",
      "Cost after iteration 400: 0.319080\n",
      "Cost after iteration 410: 0.316077\n",
      "Cost after iteration 420: 0.313169\n",
      "Cost after iteration 430: 0.310351\n",
      "Cost after iteration 440: 0.307620\n",
      "Cost after iteration 450: 0.304971\n",
      "Cost after iteration 460: 0.302400\n",
      "Cost after iteration 470: 0.299903\n",
      "Cost after iteration 480: 0.297478\n",
      "Cost after iteration 490: 0.295122\n",
      "Cost after iteration 500: 0.292830\n",
      "Cost after iteration 510: 0.290600\n",
      "Cost after iteration 520: 0.288430\n",
      "Cost after iteration 530: 0.286318\n",
      "Cost after iteration 540: 0.284260\n",
      "Cost after iteration 550: 0.282254\n",
      "Cost after iteration 560: 0.280299\n",
      "Cost after iteration 570: 0.278392\n",
      "Cost after iteration 580: 0.276532\n",
      "Cost after iteration 590: 0.274716\n",
      "Cost after iteration 600: 0.272944\n",
      "Cost after iteration 610: 0.271212\n",
      "Cost after iteration 620: 0.269521\n",
      "Cost after iteration 630: 0.267868\n",
      "Cost after iteration 640: 0.266252\n",
      "Cost after iteration 650: 0.264671\n",
      "Cost after iteration 660: 0.263125\n",
      "Cost after iteration 670: 0.261613\n",
      "Cost after iteration 680: 0.260132\n",
      "Cost after iteration 690: 0.258682\n",
      "Cost after iteration 700: 0.257262\n",
      "Cost after iteration 710: 0.255871\n",
      "Cost after iteration 720: 0.254509\n",
      "Cost after iteration 730: 0.253173\n",
      "Cost after iteration 740: 0.251863\n",
      "Cost after iteration 750: 0.250579\n",
      "Cost after iteration 760: 0.249319\n",
      "Cost after iteration 770: 0.248084\n",
      "Cost after iteration 780: 0.246871\n",
      "Cost after iteration 790: 0.245681\n",
      "Cost after iteration 800: 0.244512\n",
      "Cost after iteration 810: 0.243365\n",
      "Cost after iteration 820: 0.242238\n",
      "Cost after iteration 830: 0.241131\n",
      "Cost after iteration 840: 0.240043\n",
      "Cost after iteration 850: 0.238974\n",
      "Cost after iteration 860: 0.237924\n",
      "Cost after iteration 870: 0.236891\n",
      "Cost after iteration 880: 0.235875\n",
      "Cost after iteration 890: 0.234876\n",
      "Cost after iteration 900: 0.233893\n",
      "Cost after iteration 910: 0.232926\n",
      "Cost after iteration 920: 0.231975\n",
      "Cost after iteration 930: 0.231039\n",
      "Cost after iteration 940: 0.230117\n",
      "Cost after iteration 950: 0.229210\n",
      "Cost after iteration 960: 0.228317\n",
      "Cost after iteration 970: 0.227437\n",
      "Cost after iteration 980: 0.226570\n",
      "Cost after iteration 990: 0.225717\n",
      "Cost after iteration 1000: 0.224875\n",
      "Cost after iteration 1010: 0.224047\n",
      "Cost after iteration 1020: 0.223230\n",
      "Cost after iteration 1030: 0.222425\n",
      "Cost after iteration 1040: 0.221631\n",
      "Cost after iteration 1050: 0.220849\n",
      "Cost after iteration 1060: 0.220077\n",
      "Cost after iteration 1070: 0.219316\n",
      "Cost after iteration 1080: 0.218566\n",
      "Cost after iteration 1090: 0.217825\n",
      "Cost after iteration 1100: 0.217095\n",
      "Cost after iteration 1110: 0.216374\n",
      "Cost after iteration 1120: 0.215663\n",
      "Cost after iteration 1130: 0.214961\n",
      "Cost after iteration 1140: 0.214268\n",
      "Cost after iteration 1150: 0.213584\n",
      "Cost after iteration 1160: 0.212909\n",
      "Cost after iteration 1170: 0.212243\n",
      "Cost after iteration 1180: 0.211584\n",
      "Cost after iteration 1190: 0.210934\n",
      "Cost after iteration 1200: 0.210292\n",
      "Cost after iteration 1210: 0.209658\n",
      "Cost after iteration 1220: 0.209031\n",
      "Cost after iteration 1230: 0.208412\n",
      "Cost after iteration 1240: 0.207800\n",
      "Cost after iteration 1250: 0.207196\n",
      "Cost after iteration 1260: 0.206598\n",
      "Cost after iteration 1270: 0.206008\n",
      "Cost after iteration 1280: 0.205424\n",
      "Cost after iteration 1290: 0.204847\n",
      "Cost after iteration 1300: 0.204277\n",
      "Cost after iteration 1310: 0.203713\n",
      "Cost after iteration 1320: 0.203155\n",
      "Cost after iteration 1330: 0.202603\n",
      "Cost after iteration 1340: 0.202058\n",
      "Cost after iteration 1350: 0.201518\n",
      "Cost after iteration 1360: 0.200985\n",
      "Cost after iteration 1370: 0.200457\n",
      "Cost after iteration 1380: 0.199935\n",
      "Cost after iteration 1390: 0.199418\n",
      "Cost after iteration 1400: 0.198907\n",
      "Cost after iteration 1410: 0.198401\n",
      "Cost after iteration 1420: 0.197900\n",
      "Cost after iteration 1430: 0.197404\n",
      "Cost after iteration 1440: 0.196914\n",
      "Cost after iteration 1450: 0.196428\n",
      "Cost after iteration 1460: 0.195948\n",
      "Cost after iteration 1470: 0.195472\n",
      "Cost after iteration 1480: 0.195001\n",
      "Cost after iteration 1490: 0.194534\n",
      "Cost after iteration 1500: 0.194072\n",
      "Cost after iteration 1510: 0.193615\n",
      "Cost after iteration 1520: 0.193162\n",
      "Cost after iteration 1530: 0.192713\n",
      "Cost after iteration 1540: 0.192269\n",
      "Cost after iteration 1550: 0.191829\n",
      "Cost after iteration 1560: 0.191393\n",
      "Cost after iteration 1570: 0.190961\n",
      "Cost after iteration 1580: 0.190533\n",
      "Cost after iteration 1590: 0.190109\n",
      "Cost after iteration 1600: 0.189689\n",
      "Cost after iteration 1610: 0.189273\n",
      "Cost after iteration 1620: 0.188860\n",
      "Cost after iteration 1630: 0.188452\n",
      "Cost after iteration 1640: 0.188046\n",
      "Cost after iteration 1650: 0.187645\n",
      "Cost after iteration 1660: 0.187247\n",
      "Cost after iteration 1670: 0.186852\n",
      "Cost after iteration 1680: 0.186461\n",
      "Cost after iteration 1690: 0.186074\n",
      "Cost after iteration 1700: 0.185689\n",
      "Cost after iteration 1710: 0.185308\n",
      "Cost after iteration 1720: 0.184930\n",
      "Cost after iteration 1730: 0.184556\n",
      "Cost after iteration 1740: 0.184184\n",
      "Cost after iteration 1750: 0.183816\n",
      "Cost after iteration 1760: 0.183451\n",
      "Cost after iteration 1770: 0.183088\n",
      "Cost after iteration 1780: 0.182729\n",
      "Cost after iteration 1790: 0.182373\n",
      "Cost after iteration 1800: 0.182019\n",
      "Cost after iteration 1810: 0.181668\n",
      "Cost after iteration 1820: 0.181321\n",
      "Cost after iteration 1830: 0.180975\n",
      "Cost after iteration 1840: 0.180633\n",
      "Cost after iteration 1850: 0.180293\n",
      "Cost after iteration 1860: 0.179956\n",
      "Cost after iteration 1870: 0.179622\n",
      "Cost after iteration 1880: 0.179290\n",
      "Cost after iteration 1890: 0.178961\n",
      "Cost after iteration 1900: 0.178634\n",
      "Cost after iteration 1910: 0.178310\n",
      "Cost after iteration 1920: 0.177988\n",
      "Cost after iteration 1930: 0.177669\n",
      "Cost after iteration 1940: 0.177352\n",
      "Cost after iteration 1950: 0.177037\n",
      "Cost after iteration 1960: 0.176725\n",
      "Cost after iteration 1970: 0.176415\n",
      "Cost after iteration 1980: 0.176107\n",
      "Cost after iteration 1990: 0.175802\n",
      "Cost after iteration 2000: 0.175499\n",
      "Cost after iteration 2010: 0.175198\n",
      "Cost after iteration 2020: 0.174899\n",
      "Cost after iteration 2030: 0.174602\n",
      "Cost after iteration 2040: 0.174307\n",
      "Cost after iteration 2050: 0.174015\n",
      "Cost after iteration 2060: 0.173724\n",
      "Cost after iteration 2070: 0.173436\n",
      "Cost after iteration 2080: 0.173149\n",
      "Cost after iteration 2090: 0.172865\n",
      "Cost after iteration 2100: 0.172582\n",
      "Cost after iteration 2110: 0.172302\n",
      "Cost after iteration 2120: 0.172023\n",
      "Cost after iteration 2130: 0.171746\n",
      "Cost after iteration 2140: 0.171471\n",
      "Cost after iteration 2150: 0.171198\n",
      "Cost after iteration 2160: 0.170927\n",
      "Cost after iteration 2170: 0.170658\n",
      "Cost after iteration 2180: 0.170390\n",
      "Cost after iteration 2190: 0.170124\n",
      "Cost after iteration 2200: 0.169860\n",
      "Cost after iteration 2210: 0.169597\n",
      "Cost after iteration 2220: 0.169337\n",
      "Cost after iteration 2230: 0.169078\n",
      "Cost after iteration 2240: 0.168820\n",
      "Cost after iteration 2250: 0.168565\n",
      "Cost after iteration 2260: 0.168311\n",
      "Cost after iteration 2270: 0.168058\n",
      "Cost after iteration 2280: 0.167807\n",
      "Cost after iteration 2290: 0.167558\n",
      "Cost after iteration 2300: 0.167310\n",
      "Cost after iteration 2310: 0.167064\n",
      "Cost after iteration 2320: 0.166819\n",
      "Cost after iteration 2330: 0.166576\n",
      "Cost after iteration 2340: 0.166335\n",
      "Cost after iteration 2350: 0.166094\n",
      "Cost after iteration 2360: 0.165856\n",
      "Cost after iteration 2370: 0.165619\n",
      "Cost after iteration 2380: 0.165383\n",
      "Cost after iteration 2390: 0.165148\n",
      "Cost after iteration 2400: 0.164915\n",
      "Cost after iteration 2410: 0.164684\n",
      "Cost after iteration 2420: 0.164453\n",
      "Cost after iteration 2430: 0.164225\n",
      "Cost after iteration 2440: 0.163997\n",
      "Cost after iteration 2450: 0.163771\n",
      "Cost after iteration 2460: 0.163546\n",
      "Cost after iteration 2470: 0.163322\n",
      "Cost after iteration 2480: 0.163100\n",
      "Cost after iteration 2490: 0.162879\n",
      "Cost after iteration 2500: 0.162659\n",
      "Cost after iteration 2510: 0.162441\n",
      "Cost after iteration 2520: 0.162224\n",
      "Cost after iteration 2530: 0.162008\n",
      "Cost after iteration 2540: 0.161793\n",
      "Cost after iteration 2550: 0.161579\n",
      "Cost after iteration 2560: 0.161367\n",
      "Cost after iteration 2570: 0.161156\n",
      "Cost after iteration 2580: 0.160946\n",
      "Cost after iteration 2590: 0.160737\n",
      "Cost after iteration 2600: 0.160529\n",
      "Cost after iteration 2610: 0.160323\n",
      "Cost after iteration 2620: 0.160117\n",
      "Cost after iteration 2630: 0.159913\n",
      "Cost after iteration 2640: 0.159710\n",
      "Cost after iteration 2650: 0.159507\n",
      "Cost after iteration 2660: 0.159306\n",
      "Cost after iteration 2670: 0.159106\n",
      "Cost after iteration 2680: 0.158908\n",
      "Cost after iteration 2690: 0.158710\n",
      "Cost after iteration 2700: 0.158513\n",
      "Cost after iteration 2710: 0.158317\n",
      "Cost after iteration 2720: 0.158123\n",
      "Cost after iteration 2730: 0.157929\n",
      "Cost after iteration 2740: 0.157736\n",
      "Cost after iteration 2750: 0.157544\n",
      "Cost after iteration 2760: 0.157354\n",
      "Cost after iteration 2770: 0.157164\n",
      "Cost after iteration 2780: 0.156975\n",
      "Cost after iteration 2790: 0.156787\n",
      "Cost after iteration 2800: 0.156601\n",
      "Cost after iteration 2810: 0.156415\n",
      "Cost after iteration 2820: 0.156230\n",
      "Cost after iteration 2830: 0.156046\n",
      "Cost after iteration 2840: 0.155863\n",
      "Cost after iteration 2850: 0.155681\n",
      "Cost after iteration 2860: 0.155499\n",
      "Cost after iteration 2870: 0.155319\n",
      "Cost after iteration 2880: 0.155139\n",
      "Cost after iteration 2890: 0.154961\n",
      "Cost after iteration 2900: 0.154783\n",
      "Cost after iteration 2910: 0.154606\n",
      "Cost after iteration 2920: 0.154430\n",
      "Cost after iteration 2930: 0.154255\n",
      "Cost after iteration 2940: 0.154081\n",
      "Cost after iteration 2950: 0.153908\n",
      "Cost after iteration 2960: 0.153735\n",
      "Cost after iteration 2970: 0.153563\n",
      "Cost after iteration 2980: 0.153392\n",
      "Cost after iteration 2990: 0.153222\n",
      "Cost after iteration 3000: 0.153053\n",
      "Cost after iteration 3010: 0.152884\n",
      "Cost after iteration 3020: 0.152717\n",
      "Cost after iteration 3030: 0.152550\n",
      "Cost after iteration 3040: 0.152383\n",
      "Cost after iteration 3050: 0.152218\n",
      "Cost after iteration 3060: 0.152053\n",
      "Cost after iteration 3070: 0.151890\n",
      "Cost after iteration 3080: 0.151727\n",
      "Cost after iteration 3090: 0.151564\n",
      "Cost after iteration 3100: 0.151403\n",
      "Cost after iteration 3110: 0.151242\n",
      "Cost after iteration 3120: 0.151082\n",
      "Cost after iteration 3130: 0.150922\n",
      "Cost after iteration 3140: 0.150763\n",
      "Cost after iteration 3150: 0.150605\n",
      "Cost after iteration 3160: 0.150448\n",
      "Cost after iteration 3170: 0.150292\n",
      "Cost after iteration 3180: 0.150136\n",
      "Cost after iteration 3190: 0.149981\n",
      "Cost after iteration 3200: 0.149826\n",
      "Cost after iteration 3210: 0.149672\n",
      "Cost after iteration 3220: 0.149519\n",
      "Cost after iteration 3230: 0.149367\n",
      "Cost after iteration 3240: 0.149215\n",
      "Cost after iteration 3250: 0.149064\n",
      "Cost after iteration 3260: 0.148914\n",
      "Cost after iteration 3270: 0.148764\n",
      "Cost after iteration 3280: 0.148615\n",
      "Cost after iteration 3290: 0.148466\n",
      "Cost after iteration 3300: 0.148318\n",
      "Cost after iteration 3310: 0.148171\n",
      "Cost after iteration 3320: 0.148024\n",
      "Cost after iteration 3330: 0.147878\n",
      "Cost after iteration 3340: 0.147733\n",
      "Cost after iteration 3350: 0.147588\n",
      "Cost after iteration 3360: 0.147444\n",
      "Cost after iteration 3370: 0.147301\n",
      "Cost after iteration 3380: 0.147158\n",
      "Cost after iteration 3390: 0.147015\n",
      "Cost after iteration 3400: 0.146874\n",
      "Cost after iteration 3410: 0.146732\n",
      "Cost after iteration 3420: 0.146592\n",
      "Cost after iteration 3430: 0.146452\n",
      "Cost after iteration 3440: 0.146312\n",
      "Cost after iteration 3450: 0.146174\n",
      "Cost after iteration 3460: 0.146035\n",
      "Cost after iteration 3470: 0.145898\n",
      "Cost after iteration 3480: 0.145760\n",
      "Cost after iteration 3490: 0.145624\n",
      "Cost after iteration 3500: 0.145488\n",
      "Cost after iteration 3510: 0.145352\n",
      "Cost after iteration 3520: 0.145217\n",
      "Cost after iteration 3530: 0.145083\n",
      "Cost after iteration 3540: 0.144949\n",
      "Cost after iteration 3550: 0.144816\n",
      "Cost after iteration 3560: 0.144683\n",
      "Cost after iteration 3570: 0.144551\n",
      "Cost after iteration 3580: 0.144419\n",
      "Cost after iteration 3590: 0.144288\n",
      "Cost after iteration 3600: 0.144157\n",
      "Cost after iteration 3610: 0.144027\n",
      "Cost after iteration 3620: 0.143897\n",
      "Cost after iteration 3630: 0.143768\n",
      "Cost after iteration 3640: 0.143639\n",
      "Cost after iteration 3650: 0.143511\n",
      "Cost after iteration 3660: 0.143383\n",
      "Cost after iteration 3670: 0.143256\n",
      "Cost after iteration 3680: 0.143130\n",
      "Cost after iteration 3690: 0.143003\n",
      "Cost after iteration 3700: 0.142878\n",
      "Cost after iteration 3710: 0.142752\n",
      "Cost after iteration 3720: 0.142628\n",
      "Cost after iteration 3730: 0.142503\n",
      "Cost after iteration 3740: 0.142379\n",
      "Cost after iteration 3750: 0.142256\n",
      "Cost after iteration 3760: 0.142133\n",
      "Cost after iteration 3770: 0.142011\n",
      "Cost after iteration 3780: 0.141889\n",
      "Cost after iteration 3790: 0.141767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 3800: 0.141646\n",
      "Cost after iteration 3810: 0.141525\n",
      "Cost after iteration 3820: 0.141405\n",
      "Cost after iteration 3830: 0.141286\n",
      "Cost after iteration 3840: 0.141166\n",
      "Cost after iteration 3850: 0.141047\n",
      "Cost after iteration 3860: 0.140929\n",
      "Cost after iteration 3870: 0.140811\n",
      "Cost after iteration 3880: 0.140693\n",
      "Cost after iteration 3890: 0.140576\n",
      "Cost after iteration 3900: 0.140460\n",
      "Cost after iteration 3910: 0.140343\n",
      "Cost after iteration 3920: 0.140227\n",
      "Cost after iteration 3930: 0.140112\n",
      "Cost after iteration 3940: 0.139997\n",
      "Cost after iteration 3950: 0.139882\n",
      "Cost after iteration 3960: 0.139768\n",
      "Cost after iteration 3970: 0.139654\n",
      "Cost after iteration 3980: 0.139541\n",
      "Cost after iteration 3990: 0.139428\n",
      "Cost after iteration 4000: 0.139315\n",
      "Cost after iteration 4010: 0.139203\n",
      "Cost after iteration 4020: 0.139091\n",
      "Cost after iteration 4030: 0.138980\n",
      "Cost after iteration 4040: 0.138869\n",
      "Cost after iteration 4050: 0.138758\n",
      "Cost after iteration 4060: 0.138648\n",
      "Cost after iteration 4070: 0.138538\n",
      "Cost after iteration 4080: 0.138428\n",
      "Cost after iteration 4090: 0.138319\n",
      "Cost after iteration 4100: 0.138211\n",
      "Cost after iteration 4110: 0.138102\n",
      "Cost after iteration 4120: 0.137994\n",
      "Cost after iteration 4130: 0.137887\n",
      "Cost after iteration 4140: 0.137779\n",
      "Cost after iteration 4150: 0.137672\n",
      "Cost after iteration 4160: 0.137566\n",
      "Cost after iteration 4170: 0.137460\n",
      "Cost after iteration 4180: 0.137354\n",
      "Cost after iteration 4190: 0.137248\n",
      "Cost after iteration 4200: 0.137143\n",
      "Cost after iteration 4210: 0.137039\n",
      "Cost after iteration 4220: 0.136934\n",
      "Cost after iteration 4230: 0.136830\n",
      "Cost after iteration 4240: 0.136726\n",
      "Cost after iteration 4250: 0.136623\n",
      "Cost after iteration 4260: 0.136520\n",
      "Cost after iteration 4270: 0.136417\n",
      "Cost after iteration 4280: 0.136315\n",
      "Cost after iteration 4290: 0.136213\n",
      "Cost after iteration 4300: 0.136111\n",
      "Cost after iteration 4310: 0.136010\n",
      "Cost after iteration 4320: 0.135909\n",
      "Cost after iteration 4330: 0.135808\n",
      "Cost after iteration 4340: 0.135708\n",
      "Cost after iteration 4350: 0.135608\n",
      "Cost after iteration 4360: 0.135508\n",
      "Cost after iteration 4370: 0.135409\n",
      "Cost after iteration 4380: 0.135310\n",
      "Cost after iteration 4390: 0.135211\n",
      "Cost after iteration 4400: 0.135113\n",
      "Cost after iteration 4410: 0.135015\n",
      "Cost after iteration 4420: 0.134917\n",
      "Cost after iteration 4430: 0.134819\n",
      "Cost after iteration 4440: 0.134722\n",
      "Cost after iteration 4450: 0.134625\n",
      "Cost after iteration 4460: 0.134529\n",
      "Cost after iteration 4470: 0.134433\n",
      "Cost after iteration 4480: 0.134337\n",
      "Cost after iteration 4490: 0.134241\n",
      "Cost after iteration 4500: 0.134146\n",
      "Cost after iteration 4510: 0.134051\n",
      "Cost after iteration 4520: 0.133956\n",
      "Cost after iteration 4530: 0.133861\n",
      "Cost after iteration 4540: 0.133767\n",
      "Cost after iteration 4550: 0.133673\n",
      "Cost after iteration 4560: 0.133580\n",
      "Cost after iteration 4570: 0.133487\n",
      "Cost after iteration 4580: 0.133394\n",
      "Cost after iteration 4590: 0.133301\n",
      "Cost after iteration 4600: 0.133209\n",
      "Cost after iteration 4610: 0.133116\n",
      "Cost after iteration 4620: 0.133025\n",
      "Cost after iteration 4630: 0.132933\n",
      "Cost after iteration 4640: 0.132842\n",
      "Cost after iteration 4650: 0.132751\n",
      "Cost after iteration 4660: 0.132660\n",
      "Cost after iteration 4670: 0.132569\n",
      "Cost after iteration 4680: 0.132479\n",
      "Cost after iteration 4690: 0.132389\n",
      "Cost after iteration 4700: 0.132300\n",
      "Cost after iteration 4710: 0.132210\n",
      "Cost after iteration 4720: 0.132121\n",
      "Cost after iteration 4730: 0.132032\n",
      "Cost after iteration 4740: 0.131944\n",
      "Cost after iteration 4750: 0.131855\n",
      "Cost after iteration 4760: 0.131767\n",
      "Cost after iteration 4770: 0.131680\n",
      "Cost after iteration 4780: 0.131592\n",
      "Cost after iteration 4790: 0.131505\n",
      "Cost after iteration 4800: 0.131418\n",
      "Cost after iteration 4810: 0.131331\n",
      "Cost after iteration 4820: 0.131244\n",
      "Cost after iteration 4830: 0.131158\n",
      "Cost after iteration 4840: 0.131072\n",
      "Cost after iteration 4850: 0.130986\n",
      "Cost after iteration 4860: 0.130901\n",
      "Cost after iteration 4870: 0.130816\n",
      "Cost after iteration 4880: 0.130731\n",
      "Cost after iteration 4890: 0.130646\n",
      "Cost after iteration 4900: 0.130561\n",
      "Cost after iteration 4910: 0.130477\n",
      "Cost after iteration 4920: 0.130393\n",
      "Cost after iteration 4930: 0.130309\n",
      "Cost after iteration 4940: 0.130226\n",
      "Cost after iteration 4950: 0.130142\n",
      "Cost after iteration 4960: 0.130059\n",
      "Cost after iteration 4970: 0.129976\n",
      "Cost after iteration 4980: 0.129894\n",
      "Cost after iteration 4990: 0.129811\n",
      "Cost after iteration 5000: 0.129729\n",
      "Cost after iteration 5010: 0.129647\n",
      "Cost after iteration 5020: 0.129566\n",
      "Cost after iteration 5030: 0.129484\n",
      "Cost after iteration 5040: 0.129403\n",
      "Cost after iteration 5050: 0.129322\n",
      "Cost after iteration 5060: 0.129241\n",
      "Cost after iteration 5070: 0.129161\n",
      "Cost after iteration 5080: 0.129080\n",
      "Cost after iteration 5090: 0.129000\n",
      "Cost after iteration 5100: 0.128920\n",
      "Cost after iteration 5110: 0.128841\n",
      "Cost after iteration 5120: 0.128761\n",
      "Cost after iteration 5130: 0.128682\n",
      "Cost after iteration 5140: 0.128603\n",
      "Cost after iteration 5150: 0.128524\n",
      "Cost after iteration 5160: 0.128446\n",
      "Cost after iteration 5170: 0.128367\n",
      "Cost after iteration 5180: 0.128289\n",
      "Cost after iteration 5190: 0.128211\n",
      "Cost after iteration 5200: 0.128134\n",
      "Cost after iteration 5210: 0.128056\n",
      "Cost after iteration 5220: 0.127979\n",
      "Cost after iteration 5230: 0.127902\n",
      "Cost after iteration 5240: 0.127825\n",
      "Cost after iteration 5250: 0.127748\n",
      "Cost after iteration 5260: 0.127672\n",
      "Cost after iteration 5270: 0.127596\n",
      "Cost after iteration 5280: 0.127519\n",
      "Cost after iteration 5290: 0.127444\n",
      "Cost after iteration 5300: 0.127368\n",
      "Cost after iteration 5310: 0.127293\n",
      "Cost after iteration 5320: 0.127217\n",
      "Cost after iteration 5330: 0.127142\n",
      "Cost after iteration 5340: 0.127067\n",
      "Cost after iteration 5350: 0.126993\n",
      "Cost after iteration 5360: 0.126918\n",
      "Cost after iteration 5370: 0.126844\n",
      "Cost after iteration 5380: 0.126770\n",
      "Cost after iteration 5390: 0.126696\n",
      "Cost after iteration 5400: 0.126623\n",
      "Cost after iteration 5410: 0.126549\n",
      "Cost after iteration 5420: 0.126476\n",
      "Cost after iteration 5430: 0.126403\n",
      "Cost after iteration 5440: 0.126330\n",
      "Cost after iteration 5450: 0.126257\n",
      "Cost after iteration 5460: 0.126185\n",
      "Cost after iteration 5470: 0.126112\n",
      "Cost after iteration 5480: 0.126040\n",
      "Cost after iteration 5490: 0.125968\n",
      "Cost after iteration 5500: 0.125896\n",
      "Cost after iteration 5510: 0.125825\n",
      "Cost after iteration 5520: 0.125753\n",
      "Cost after iteration 5530: 0.125682\n",
      "Cost after iteration 5540: 0.125611\n",
      "Cost after iteration 5550: 0.125540\n",
      "Cost after iteration 5560: 0.125469\n",
      "Cost after iteration 5570: 0.125399\n",
      "Cost after iteration 5580: 0.125329\n",
      "Cost after iteration 5590: 0.125258\n",
      "Cost after iteration 5600: 0.125188\n",
      "Cost after iteration 5610: 0.125119\n",
      "Cost after iteration 5620: 0.125049\n",
      "Cost after iteration 5630: 0.124980\n",
      "Cost after iteration 5640: 0.124910\n",
      "Cost after iteration 5650: 0.124841\n",
      "Cost after iteration 5660: 0.124772\n",
      "Cost after iteration 5670: 0.124704\n",
      "Cost after iteration 5680: 0.124635\n",
      "Cost after iteration 5690: 0.124567\n",
      "Cost after iteration 5700: 0.124498\n",
      "Cost after iteration 5710: 0.124430\n",
      "Cost after iteration 5720: 0.124362\n",
      "Cost after iteration 5730: 0.124295\n",
      "Cost after iteration 5740: 0.124227\n",
      "Cost after iteration 5750: 0.124160\n",
      "Cost after iteration 5760: 0.124092\n",
      "Cost after iteration 5770: 0.124025\n",
      "Cost after iteration 5780: 0.123958\n",
      "Cost after iteration 5790: 0.123892\n",
      "Cost after iteration 5800: 0.123825\n",
      "Cost after iteration 5810: 0.123759\n",
      "Cost after iteration 5820: 0.123692\n",
      "Cost after iteration 5830: 0.123626\n",
      "Cost after iteration 5840: 0.123560\n",
      "Cost after iteration 5850: 0.123495\n",
      "Cost after iteration 5860: 0.123429\n",
      "Cost after iteration 5870: 0.123364\n",
      "Cost after iteration 5880: 0.123298\n",
      "Cost after iteration 5890: 0.123233\n",
      "Cost after iteration 5900: 0.123168\n",
      "Cost after iteration 5910: 0.123103\n",
      "Cost after iteration 5920: 0.123039\n",
      "Cost after iteration 5930: 0.122974\n",
      "Cost after iteration 5940: 0.122910\n",
      "Cost after iteration 5950: 0.122846\n",
      "Cost after iteration 5960: 0.122782\n",
      "Cost after iteration 5970: 0.122718\n",
      "Cost after iteration 5980: 0.122654\n",
      "Cost after iteration 5990: 0.122590\n",
      "Cost after iteration 6000: 0.122527\n",
      "Cost after iteration 6010: 0.122463\n",
      "Cost after iteration 6020: 0.122400\n",
      "Cost after iteration 6030: 0.122337\n",
      "Cost after iteration 6040: 0.122274\n",
      "Cost after iteration 6050: 0.122212\n",
      "Cost after iteration 6060: 0.122149\n",
      "Cost after iteration 6070: 0.122087\n",
      "Cost after iteration 6080: 0.122024\n",
      "Cost after iteration 6090: 0.121962\n",
      "Cost after iteration 6100: 0.121900\n",
      "Cost after iteration 6110: 0.121839\n",
      "Cost after iteration 6120: 0.121777\n",
      "Cost after iteration 6130: 0.121715\n",
      "Cost after iteration 6140: 0.121654\n",
      "Cost after iteration 6150: 0.121593\n",
      "Cost after iteration 6160: 0.121531\n",
      "Cost after iteration 6170: 0.121470\n",
      "Cost after iteration 6180: 0.121410\n",
      "Cost after iteration 6190: 0.121349\n",
      "Cost after iteration 6200: 0.121288\n",
      "Cost after iteration 6210: 0.121228\n",
      "Cost after iteration 6220: 0.121168\n",
      "Cost after iteration 6230: 0.121107\n",
      "Cost after iteration 6240: 0.121047\n",
      "Cost after iteration 6250: 0.120987\n",
      "Cost after iteration 6260: 0.120928\n",
      "Cost after iteration 6270: 0.120868\n",
      "Cost after iteration 6280: 0.120809\n",
      "Cost after iteration 6290: 0.120749\n",
      "Cost after iteration 6300: 0.120690\n",
      "Cost after iteration 6310: 0.120631\n",
      "Cost after iteration 6320: 0.120572\n",
      "Cost after iteration 6330: 0.120513\n",
      "Cost after iteration 6340: 0.120455\n",
      "Cost after iteration 6350: 0.120396\n",
      "Cost after iteration 6360: 0.120338\n",
      "Cost after iteration 6370: 0.120279\n",
      "Cost after iteration 6380: 0.120221\n",
      "Cost after iteration 6390: 0.120163\n",
      "Cost after iteration 6400: 0.120105\n",
      "Cost after iteration 6410: 0.120047\n",
      "Cost after iteration 6420: 0.119990\n",
      "Cost after iteration 6430: 0.119932\n",
      "Cost after iteration 6440: 0.119875\n",
      "Cost after iteration 6450: 0.119818\n",
      "Cost after iteration 6460: 0.119760\n",
      "Cost after iteration 6470: 0.119703\n",
      "Cost after iteration 6480: 0.119646\n",
      "Cost after iteration 6490: 0.119590\n",
      "Cost after iteration 6500: 0.119533\n",
      "Cost after iteration 6510: 0.119477\n",
      "Cost after iteration 6520: 0.119420\n",
      "Cost after iteration 6530: 0.119364\n",
      "Cost after iteration 6540: 0.119308\n",
      "Cost after iteration 6550: 0.119252\n",
      "Cost after iteration 6560: 0.119196\n",
      "Cost after iteration 6570: 0.119140\n",
      "Cost after iteration 6580: 0.119084\n",
      "Cost after iteration 6590: 0.119029\n",
      "Cost after iteration 6600: 0.118973\n",
      "Cost after iteration 6610: 0.118918\n",
      "Cost after iteration 6620: 0.118863\n",
      "Cost after iteration 6630: 0.118808\n",
      "Cost after iteration 6640: 0.118753\n",
      "Cost after iteration 6650: 0.118698\n",
      "Cost after iteration 6660: 0.118643\n",
      "Cost after iteration 6670: 0.118589\n",
      "Cost after iteration 6680: 0.118534\n",
      "Cost after iteration 6690: 0.118480\n",
      "Cost after iteration 6700: 0.118425\n",
      "Cost after iteration 6710: 0.118371\n",
      "Cost after iteration 6720: 0.118317\n",
      "Cost after iteration 6730: 0.118263\n",
      "Cost after iteration 6740: 0.118209\n",
      "Cost after iteration 6750: 0.118156\n",
      "Cost after iteration 6760: 0.118102\n",
      "Cost after iteration 6770: 0.118049\n",
      "Cost after iteration 6780: 0.117995\n",
      "Cost after iteration 6790: 0.117942\n",
      "Cost after iteration 6800: 0.117889\n",
      "Cost after iteration 6810: 0.117836\n",
      "Cost after iteration 6820: 0.117783\n",
      "Cost after iteration 6830: 0.117730\n",
      "Cost after iteration 6840: 0.117678\n",
      "Cost after iteration 6850: 0.117625\n",
      "Cost after iteration 6860: 0.117573\n",
      "Cost after iteration 6870: 0.117520\n",
      "Cost after iteration 6880: 0.117468\n",
      "Cost after iteration 6890: 0.117416\n",
      "Cost after iteration 6900: 0.117364\n",
      "Cost after iteration 6910: 0.117312\n",
      "Cost after iteration 6920: 0.117260\n",
      "Cost after iteration 6930: 0.117208\n",
      "Cost after iteration 6940: 0.117157\n",
      "Cost after iteration 6950: 0.117105\n",
      "Cost after iteration 6960: 0.117054\n",
      "Cost after iteration 6970: 0.117002\n",
      "Cost after iteration 6980: 0.116951\n",
      "Cost after iteration 6990: 0.116900\n",
      "Cost after iteration 7000: 0.116849\n",
      "Cost after iteration 7010: 0.116798\n",
      "Cost after iteration 7020: 0.116747\n",
      "Cost after iteration 7030: 0.116697\n",
      "Cost after iteration 7040: 0.116646\n",
      "Cost after iteration 7050: 0.116596\n",
      "Cost after iteration 7060: 0.116545\n",
      "Cost after iteration 7070: 0.116495\n",
      "Cost after iteration 7080: 0.116445\n",
      "Cost after iteration 7090: 0.116395\n",
      "Cost after iteration 7100: 0.116345\n",
      "Cost after iteration 7110: 0.116295\n",
      "Cost after iteration 7120: 0.116245\n",
      "Cost after iteration 7130: 0.116195\n",
      "Cost after iteration 7140: 0.116146\n",
      "Cost after iteration 7150: 0.116096\n",
      "Cost after iteration 7160: 0.116047\n",
      "Cost after iteration 7170: 0.115998\n",
      "Cost after iteration 7180: 0.115948\n",
      "Cost after iteration 7190: 0.115899\n",
      "Cost after iteration 7200: 0.115850\n",
      "Cost after iteration 7210: 0.115801\n",
      "Cost after iteration 7220: 0.115753\n",
      "Cost after iteration 7230: 0.115704\n",
      "Cost after iteration 7240: 0.115655\n",
      "Cost after iteration 7250: 0.115607\n",
      "Cost after iteration 7260: 0.115558\n",
      "Cost after iteration 7270: 0.115510\n",
      "Cost after iteration 7280: 0.115462\n",
      "Cost after iteration 7290: 0.115414\n",
      "Cost after iteration 7300: 0.115366\n",
      "Cost after iteration 7310: 0.115318\n",
      "Cost after iteration 7320: 0.115270\n",
      "Cost after iteration 7330: 0.115222\n",
      "Cost after iteration 7340: 0.115174\n",
      "Cost after iteration 7350: 0.115127\n",
      "Cost after iteration 7360: 0.115079\n",
      "Cost after iteration 7370: 0.115032\n",
      "Cost after iteration 7380: 0.114985\n",
      "Cost after iteration 7390: 0.114937\n",
      "Cost after iteration 7400: 0.114890\n",
      "Cost after iteration 7410: 0.114843\n",
      "Cost after iteration 7420: 0.114796\n",
      "Cost after iteration 7430: 0.114749\n",
      "Cost after iteration 7440: 0.114703\n",
      "Cost after iteration 7450: 0.114656\n",
      "Cost after iteration 7460: 0.114609\n",
      "Cost after iteration 7470: 0.114563\n",
      "Cost after iteration 7480: 0.114517\n",
      "Cost after iteration 7490: 0.114470\n",
      "Cost after iteration 7500: 0.114424\n",
      "Cost after iteration 7510: 0.114378\n",
      "Cost after iteration 7520: 0.114332\n",
      "Cost after iteration 7530: 0.114286\n",
      "Cost after iteration 7540: 0.114240\n",
      "Cost after iteration 7550: 0.114194\n",
      "Cost after iteration 7560: 0.114148\n",
      "Cost after iteration 7570: 0.114103\n",
      "Cost after iteration 7580: 0.114057\n",
      "Cost after iteration 7590: 0.114012\n",
      "Cost after iteration 7600: 0.113966\n",
      "Cost after iteration 7610: 0.113921\n",
      "Cost after iteration 7620: 0.113876\n",
      "Cost after iteration 7630: 0.113831\n",
      "Cost after iteration 7640: 0.113786\n",
      "Cost after iteration 7650: 0.113741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 7660: 0.113696\n",
      "Cost after iteration 7670: 0.113651\n",
      "Cost after iteration 7680: 0.113606\n",
      "Cost after iteration 7690: 0.113562\n",
      "Cost after iteration 7700: 0.113517\n",
      "Cost after iteration 7710: 0.113473\n",
      "Cost after iteration 7720: 0.113428\n",
      "Cost after iteration 7730: 0.113384\n",
      "Cost after iteration 7740: 0.113340\n",
      "Cost after iteration 7750: 0.113296\n",
      "Cost after iteration 7760: 0.113252\n",
      "Cost after iteration 7770: 0.113208\n",
      "Cost after iteration 7780: 0.113164\n",
      "Cost after iteration 7790: 0.113120\n",
      "Cost after iteration 7800: 0.113076\n",
      "Cost after iteration 7810: 0.113033\n",
      "Cost after iteration 7820: 0.112989\n",
      "Cost after iteration 7830: 0.112946\n",
      "Cost after iteration 7840: 0.112902\n",
      "Cost after iteration 7850: 0.112859\n",
      "Cost after iteration 7860: 0.112816\n",
      "Cost after iteration 7870: 0.112772\n",
      "Cost after iteration 7880: 0.112729\n",
      "Cost after iteration 7890: 0.112686\n",
      "Cost after iteration 7900: 0.112643\n",
      "Cost after iteration 7910: 0.112600\n",
      "Cost after iteration 7920: 0.112558\n",
      "Cost after iteration 7930: 0.112515\n",
      "Cost after iteration 7940: 0.112472\n",
      "Cost after iteration 7950: 0.112430\n",
      "Cost after iteration 7960: 0.112387\n",
      "Cost after iteration 7970: 0.112345\n",
      "Cost after iteration 7980: 0.112302\n",
      "Cost after iteration 7990: 0.112260\n",
      "Cost after iteration 8000: 0.112218\n",
      "Cost after iteration 8010: 0.112176\n",
      "Cost after iteration 8020: 0.112134\n",
      "Cost after iteration 8030: 0.112092\n",
      "Cost after iteration 8040: 0.112050\n",
      "Cost after iteration 8050: 0.112008\n",
      "Cost after iteration 8060: 0.111966\n",
      "Cost after iteration 8070: 0.111925\n",
      "Cost after iteration 8080: 0.111883\n",
      "Cost after iteration 8090: 0.111842\n",
      "Cost after iteration 8100: 0.111800\n",
      "Cost after iteration 8110: 0.111759\n",
      "Cost after iteration 8120: 0.111718\n",
      "Cost after iteration 8130: 0.111676\n",
      "Cost after iteration 8140: 0.111635\n",
      "Cost after iteration 8150: 0.111594\n",
      "Cost after iteration 8160: 0.111553\n",
      "Cost after iteration 8170: 0.111512\n",
      "Cost after iteration 8180: 0.111471\n",
      "Cost after iteration 8190: 0.111430\n",
      "Cost after iteration 8200: 0.111390\n",
      "Cost after iteration 8210: 0.111349\n",
      "Cost after iteration 8220: 0.111308\n",
      "Cost after iteration 8230: 0.111268\n",
      "Cost after iteration 8240: 0.111227\n",
      "Cost after iteration 8250: 0.111187\n",
      "Cost after iteration 8260: 0.111147\n",
      "Cost after iteration 8270: 0.111106\n",
      "Cost after iteration 8280: 0.111066\n",
      "Cost after iteration 8290: 0.111026\n",
      "Cost after iteration 8300: 0.110986\n",
      "Cost after iteration 8310: 0.110946\n",
      "Cost after iteration 8320: 0.110906\n",
      "Cost after iteration 8330: 0.110866\n",
      "Cost after iteration 8340: 0.110827\n",
      "Cost after iteration 8350: 0.110787\n",
      "Cost after iteration 8360: 0.110747\n",
      "Cost after iteration 8370: 0.110708\n",
      "Cost after iteration 8380: 0.110668\n",
      "Cost after iteration 8390: 0.110629\n",
      "Cost after iteration 8400: 0.110589\n",
      "Cost after iteration 8410: 0.110550\n",
      "Cost after iteration 8420: 0.110511\n",
      "Cost after iteration 8430: 0.110472\n",
      "Cost after iteration 8440: 0.110433\n",
      "Cost after iteration 8450: 0.110394\n",
      "Cost after iteration 8460: 0.110355\n",
      "Cost after iteration 8470: 0.110316\n",
      "Cost after iteration 8480: 0.110277\n",
      "Cost after iteration 8490: 0.110238\n",
      "Cost after iteration 8500: 0.110199\n",
      "Cost after iteration 8510: 0.110161\n",
      "Cost after iteration 8520: 0.110122\n",
      "Cost after iteration 8530: 0.110084\n",
      "Cost after iteration 8540: 0.110045\n",
      "Cost after iteration 8550: 0.110007\n",
      "Cost after iteration 8560: 0.109968\n",
      "Cost after iteration 8570: 0.109930\n",
      "Cost after iteration 8580: 0.109892\n",
      "Cost after iteration 8590: 0.109854\n",
      "Cost after iteration 8600: 0.109816\n",
      "Cost after iteration 8610: 0.109778\n",
      "Cost after iteration 8620: 0.109740\n",
      "Cost after iteration 8630: 0.109702\n",
      "Cost after iteration 8640: 0.109664\n",
      "Cost after iteration 8650: 0.109626\n",
      "Cost after iteration 8660: 0.109589\n",
      "Cost after iteration 8670: 0.109551\n",
      "Cost after iteration 8680: 0.109513\n",
      "Cost after iteration 8690: 0.109476\n",
      "Cost after iteration 8700: 0.109438\n",
      "Cost after iteration 8710: 0.109401\n",
      "Cost after iteration 8720: 0.109364\n",
      "Cost after iteration 8730: 0.109326\n",
      "Cost after iteration 8740: 0.109289\n",
      "Cost after iteration 8750: 0.109252\n",
      "Cost after iteration 8760: 0.109215\n",
      "Cost after iteration 8770: 0.109178\n",
      "Cost after iteration 8780: 0.109141\n",
      "Cost after iteration 8790: 0.109104\n",
      "Cost after iteration 8800: 0.109067\n",
      "Cost after iteration 8810: 0.109030\n",
      "Cost after iteration 8820: 0.108994\n",
      "Cost after iteration 8830: 0.108957\n",
      "Cost after iteration 8840: 0.108920\n",
      "Cost after iteration 8850: 0.108884\n",
      "Cost after iteration 8860: 0.108847\n",
      "Cost after iteration 8870: 0.108811\n",
      "Cost after iteration 8880: 0.108774\n",
      "Cost after iteration 8890: 0.108738\n",
      "Cost after iteration 8900: 0.108702\n",
      "Cost after iteration 8910: 0.108666\n",
      "Cost after iteration 8920: 0.108630\n",
      "Cost after iteration 8930: 0.108593\n",
      "Cost after iteration 8940: 0.108557\n",
      "Cost after iteration 8950: 0.108521\n",
      "Cost after iteration 8960: 0.108485\n",
      "Cost after iteration 8970: 0.108450\n",
      "Cost after iteration 8980: 0.108414\n",
      "Cost after iteration 8990: 0.108378\n",
      "Cost after iteration 9000: 0.108342\n",
      "Cost after iteration 9010: 0.108307\n",
      "Cost after iteration 9020: 0.108271\n",
      "Cost after iteration 9030: 0.108236\n",
      "Cost after iteration 9040: 0.108200\n",
      "Cost after iteration 9050: 0.108165\n",
      "Cost after iteration 9060: 0.108129\n",
      "Cost after iteration 9070: 0.108094\n",
      "Cost after iteration 9080: 0.108059\n",
      "Cost after iteration 9090: 0.108024\n",
      "Cost after iteration 9100: 0.107988\n",
      "Cost after iteration 9110: 0.107953\n",
      "Cost after iteration 9120: 0.107918\n",
      "Cost after iteration 9130: 0.107883\n",
      "Cost after iteration 9140: 0.107848\n",
      "Cost after iteration 9150: 0.107814\n",
      "Cost after iteration 9160: 0.107779\n",
      "Cost after iteration 9170: 0.107744\n",
      "Cost after iteration 9180: 0.107709\n",
      "Cost after iteration 9190: 0.107675\n",
      "Cost after iteration 9200: 0.107640\n",
      "Cost after iteration 9210: 0.107605\n",
      "Cost after iteration 9220: 0.107571\n",
      "Cost after iteration 9230: 0.107536\n",
      "Cost after iteration 9240: 0.107502\n",
      "Cost after iteration 9250: 0.107468\n",
      "Cost after iteration 9260: 0.107433\n",
      "Cost after iteration 9270: 0.107399\n",
      "Cost after iteration 9280: 0.107365\n",
      "Cost after iteration 9290: 0.107331\n",
      "Cost after iteration 9300: 0.107297\n",
      "Cost after iteration 9310: 0.107263\n",
      "Cost after iteration 9320: 0.107229\n",
      "Cost after iteration 9330: 0.107195\n",
      "Cost after iteration 9340: 0.107161\n",
      "Cost after iteration 9350: 0.107127\n",
      "Cost after iteration 9360: 0.107093\n",
      "Cost after iteration 9370: 0.107060\n",
      "Cost after iteration 9380: 0.107026\n",
      "Cost after iteration 9390: 0.106992\n",
      "Cost after iteration 9400: 0.106959\n",
      "Cost after iteration 9410: 0.106925\n",
      "Cost after iteration 9420: 0.106892\n",
      "Cost after iteration 9430: 0.106858\n",
      "Cost after iteration 9440: 0.106825\n",
      "Cost after iteration 9450: 0.106792\n",
      "Cost after iteration 9460: 0.106758\n",
      "Cost after iteration 9470: 0.106725\n",
      "Cost after iteration 9480: 0.106692\n",
      "Cost after iteration 9490: 0.106659\n",
      "Cost after iteration 9500: 0.106626\n",
      "Cost after iteration 9510: 0.106593\n",
      "Cost after iteration 9520: 0.106560\n",
      "Cost after iteration 9530: 0.106527\n",
      "Cost after iteration 9540: 0.106494\n",
      "Cost after iteration 9550: 0.106461\n",
      "Cost after iteration 9560: 0.106429\n",
      "Cost after iteration 9570: 0.106396\n",
      "Cost after iteration 9580: 0.106363\n",
      "Cost after iteration 9590: 0.106330\n",
      "Cost after iteration 9600: 0.106298\n",
      "Cost after iteration 9610: 0.106265\n",
      "Cost after iteration 9620: 0.106233\n",
      "Cost after iteration 9630: 0.106200\n",
      "Cost after iteration 9640: 0.106168\n",
      "Cost after iteration 9650: 0.106136\n",
      "Cost after iteration 9660: 0.106103\n",
      "Cost after iteration 9670: 0.106071\n",
      "Cost after iteration 9680: 0.106039\n",
      "Cost after iteration 9690: 0.106007\n",
      "Cost after iteration 9700: 0.105975\n",
      "Cost after iteration 9710: 0.105943\n",
      "Cost after iteration 9720: 0.105911\n",
      "Cost after iteration 9730: 0.105879\n",
      "Cost after iteration 9740: 0.105847\n",
      "Cost after iteration 9750: 0.105815\n",
      "Cost after iteration 9760: 0.105783\n",
      "Cost after iteration 9770: 0.105751\n",
      "Cost after iteration 9780: 0.105719\n",
      "Cost after iteration 9790: 0.105688\n",
      "Cost after iteration 9800: 0.105656\n",
      "Cost after iteration 9810: 0.105625\n",
      "Cost after iteration 9820: 0.105593\n",
      "Cost after iteration 9830: 0.105561\n",
      "Cost after iteration 9840: 0.105530\n",
      "Cost after iteration 9850: 0.105499\n",
      "Cost after iteration 9860: 0.105467\n",
      "Cost after iteration 9870: 0.105436\n",
      "Cost after iteration 9880: 0.105405\n",
      "Cost after iteration 9890: 0.105373\n",
      "Cost after iteration 9900: 0.105342\n",
      "Cost after iteration 9910: 0.105311\n",
      "Cost after iteration 9920: 0.105280\n",
      "Cost after iteration 9930: 0.105249\n",
      "Cost after iteration 9940: 0.105218\n",
      "Cost after iteration 9950: 0.105187\n",
      "Cost after iteration 9960: 0.105156\n",
      "Cost after iteration 9970: 0.105125\n",
      "Cost after iteration 9980: 0.105094\n",
      "Cost after iteration 9990: 0.105063\n",
      "Cost after iteration 10000: 0.105033\n",
      "Cost after iteration 10010: 0.105002\n",
      "Cost after iteration 10020: 0.104971\n",
      "Cost after iteration 10030: 0.104941\n",
      "Cost after iteration 10040: 0.104910\n",
      "Cost after iteration 10050: 0.104879\n",
      "Cost after iteration 10060: 0.104849\n",
      "Cost after iteration 10070: 0.104819\n",
      "Cost after iteration 10080: 0.104788\n",
      "Cost after iteration 10090: 0.104758\n",
      "Cost after iteration 10100: 0.104727\n",
      "Cost after iteration 10110: 0.104697\n",
      "Cost after iteration 10120: 0.104667\n",
      "Cost after iteration 10130: 0.104637\n",
      "Cost after iteration 10140: 0.104606\n",
      "Cost after iteration 10150: 0.104576\n",
      "Cost after iteration 10160: 0.104546\n",
      "Cost after iteration 10170: 0.104516\n",
      "Cost after iteration 10180: 0.104486\n",
      "Cost after iteration 10190: 0.104456\n",
      "Cost after iteration 10200: 0.104426\n",
      "Cost after iteration 10210: 0.104397\n",
      "Cost after iteration 10220: 0.104367\n",
      "Cost after iteration 10230: 0.104337\n",
      "Cost after iteration 10240: 0.104307\n",
      "Cost after iteration 10250: 0.104277\n",
      "Cost after iteration 10260: 0.104248\n",
      "Cost after iteration 10270: 0.104218\n",
      "Cost after iteration 10280: 0.104189\n",
      "Cost after iteration 10290: 0.104159\n",
      "Cost after iteration 10300: 0.104130\n",
      "Cost after iteration 10310: 0.104100\n",
      "Cost after iteration 10320: 0.104071\n",
      "Cost after iteration 10330: 0.104041\n",
      "Cost after iteration 10340: 0.104012\n",
      "Cost after iteration 10350: 0.103983\n",
      "Cost after iteration 10360: 0.103953\n",
      "Cost after iteration 10370: 0.103924\n",
      "Cost after iteration 10380: 0.103895\n",
      "Cost after iteration 10390: 0.103866\n",
      "Cost after iteration 10400: 0.103837\n",
      "Cost after iteration 10410: 0.103808\n",
      "Cost after iteration 10420: 0.103779\n",
      "Cost after iteration 10430: 0.103750\n",
      "Cost after iteration 10440: 0.103721\n",
      "Cost after iteration 10450: 0.103692\n",
      "Cost after iteration 10460: 0.103663\n",
      "Cost after iteration 10470: 0.103634\n",
      "Cost after iteration 10480: 0.103605\n",
      "Cost after iteration 10490: 0.103577\n",
      "Cost after iteration 10500: 0.103548\n",
      "Cost after iteration 10510: 0.103519\n",
      "Cost after iteration 10520: 0.103491\n",
      "Cost after iteration 10530: 0.103462\n",
      "Cost after iteration 10540: 0.103433\n",
      "Cost after iteration 10550: 0.103405\n",
      "Cost after iteration 10560: 0.103376\n",
      "Cost after iteration 10570: 0.103348\n",
      "Cost after iteration 10580: 0.103320\n",
      "Cost after iteration 10590: 0.103291\n",
      "Cost after iteration 10600: 0.103263\n",
      "Cost after iteration 10610: 0.103235\n",
      "Cost after iteration 10620: 0.103206\n",
      "Cost after iteration 10630: 0.103178\n",
      "Cost after iteration 10640: 0.103150\n",
      "Cost after iteration 10650: 0.103122\n",
      "Cost after iteration 10660: 0.103094\n",
      "Cost after iteration 10670: 0.103066\n",
      "Cost after iteration 10680: 0.103038\n",
      "Cost after iteration 10690: 0.103010\n",
      "Cost after iteration 10700: 0.102982\n",
      "Cost after iteration 10710: 0.102954\n",
      "Cost after iteration 10720: 0.102926\n",
      "Cost after iteration 10730: 0.102898\n",
      "Cost after iteration 10740: 0.102870\n",
      "Cost after iteration 10750: 0.102842\n",
      "Cost after iteration 10760: 0.102815\n",
      "Cost after iteration 10770: 0.102787\n",
      "Cost after iteration 10780: 0.102759\n",
      "Cost after iteration 10790: 0.102732\n",
      "Cost after iteration 10800: 0.102704\n",
      "Cost after iteration 10810: 0.102676\n",
      "Cost after iteration 10820: 0.102649\n",
      "Cost after iteration 10830: 0.102621\n",
      "Cost after iteration 10840: 0.102594\n",
      "Cost after iteration 10850: 0.102567\n",
      "Cost after iteration 10860: 0.102539\n",
      "Cost after iteration 10870: 0.102512\n",
      "Cost after iteration 10880: 0.102484\n",
      "Cost after iteration 10890: 0.102457\n",
      "Cost after iteration 10900: 0.102430\n",
      "Cost after iteration 10910: 0.102403\n",
      "Cost after iteration 10920: 0.102376\n",
      "Cost after iteration 10930: 0.102348\n",
      "Cost after iteration 10940: 0.102321\n",
      "Cost after iteration 10950: 0.102294\n",
      "Cost after iteration 10960: 0.102267\n",
      "Cost after iteration 10970: 0.102240\n",
      "Cost after iteration 10980: 0.102213\n",
      "Cost after iteration 10990: 0.102186\n",
      "Cost after iteration 11000: 0.102159\n",
      "Cost after iteration 11010: 0.102133\n",
      "Cost after iteration 11020: 0.102106\n",
      "Cost after iteration 11030: 0.102079\n",
      "Cost after iteration 11040: 0.102052\n",
      "Cost after iteration 11050: 0.102026\n",
      "Cost after iteration 11060: 0.101999\n",
      "Cost after iteration 11070: 0.101972\n",
      "Cost after iteration 11080: 0.101946\n",
      "Cost after iteration 11090: 0.101919\n",
      "Cost after iteration 11100: 0.101892\n",
      "Cost after iteration 11110: 0.101866\n",
      "Cost after iteration 11120: 0.101839\n",
      "Cost after iteration 11130: 0.101813\n",
      "Cost after iteration 11140: 0.101787\n",
      "Cost after iteration 11150: 0.101760\n",
      "Cost after iteration 11160: 0.101734\n",
      "Cost after iteration 11170: 0.101707\n",
      "Cost after iteration 11180: 0.101681\n",
      "Cost after iteration 11190: 0.101655\n",
      "Cost after iteration 11200: 0.101629\n",
      "Cost after iteration 11210: 0.101603\n",
      "Cost after iteration 11220: 0.101576\n",
      "Cost after iteration 11230: 0.101550\n",
      "Cost after iteration 11240: 0.101524\n",
      "Cost after iteration 11250: 0.101498\n",
      "Cost after iteration 11260: 0.101472\n",
      "Cost after iteration 11270: 0.101446\n",
      "Cost after iteration 11280: 0.101420\n",
      "Cost after iteration 11290: 0.101394\n",
      "Cost after iteration 11300: 0.101368\n",
      "Cost after iteration 11310: 0.101342\n",
      "Cost after iteration 11320: 0.101317\n",
      "Cost after iteration 11330: 0.101291\n",
      "Cost after iteration 11340: 0.101265\n",
      "Cost after iteration 11350: 0.101239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 11360: 0.101214\n",
      "Cost after iteration 11370: 0.101188\n",
      "Cost after iteration 11380: 0.101162\n",
      "Cost after iteration 11390: 0.101137\n",
      "Cost after iteration 11400: 0.101111\n",
      "Cost after iteration 11410: 0.101085\n",
      "Cost after iteration 11420: 0.101060\n",
      "Cost after iteration 11430: 0.101034\n",
      "Cost after iteration 11440: 0.101009\n",
      "Cost after iteration 11450: 0.100984\n",
      "Cost after iteration 11460: 0.100958\n",
      "Cost after iteration 11470: 0.100933\n",
      "Cost after iteration 11480: 0.100908\n",
      "Cost after iteration 11490: 0.100882\n",
      "Cost after iteration 11500: 0.100857\n",
      "Cost after iteration 11510: 0.100832\n",
      "Cost after iteration 11520: 0.100807\n",
      "Cost after iteration 11530: 0.100781\n",
      "Cost after iteration 11540: 0.100756\n",
      "Cost after iteration 11550: 0.100731\n",
      "Cost after iteration 11560: 0.100706\n",
      "Cost after iteration 11570: 0.100681\n",
      "Cost after iteration 11580: 0.100656\n",
      "Cost after iteration 11590: 0.100631\n",
      "Cost after iteration 11600: 0.100606\n",
      "Cost after iteration 11610: 0.100581\n",
      "Cost after iteration 11620: 0.100556\n",
      "Cost after iteration 11630: 0.100531\n",
      "Cost after iteration 11640: 0.100506\n",
      "Cost after iteration 11650: 0.100482\n",
      "Cost after iteration 11660: 0.100457\n",
      "Cost after iteration 11670: 0.100432\n",
      "Cost after iteration 11680: 0.100407\n",
      "Cost after iteration 11690: 0.100383\n",
      "Cost after iteration 11700: 0.100358\n",
      "Cost after iteration 11710: 0.100333\n",
      "Cost after iteration 11720: 0.100309\n",
      "Cost after iteration 11730: 0.100284\n",
      "Cost after iteration 11740: 0.100260\n",
      "Cost after iteration 11750: 0.100235\n",
      "Cost after iteration 11760: 0.100211\n",
      "Cost after iteration 11770: 0.100186\n",
      "Cost after iteration 11780: 0.100162\n",
      "Cost after iteration 11790: 0.100137\n",
      "Cost after iteration 11800: 0.100113\n",
      "Cost after iteration 11810: 0.100089\n",
      "Cost after iteration 11820: 0.100064\n",
      "Cost after iteration 11830: 0.100040\n",
      "Cost after iteration 11840: 0.100016\n",
      "Cost after iteration 11850: 0.099991\n",
      "Cost after iteration 11860: 0.099967\n",
      "Cost after iteration 11870: 0.099943\n",
      "Cost after iteration 11880: 0.099919\n",
      "Cost after iteration 11890: 0.099895\n",
      "Cost after iteration 11900: 0.099871\n",
      "Cost after iteration 11910: 0.099847\n",
      "Cost after iteration 11920: 0.099823\n",
      "Cost after iteration 11930: 0.099799\n",
      "Cost after iteration 11940: 0.099775\n",
      "Cost after iteration 11950: 0.099751\n",
      "Cost after iteration 11960: 0.099727\n",
      "Cost after iteration 11970: 0.099703\n",
      "Cost after iteration 11980: 0.099679\n",
      "Cost after iteration 11990: 0.099655\n",
      "Cost after iteration 12000: 0.099631\n",
      "Cost after iteration 12010: 0.099608\n",
      "Cost after iteration 12020: 0.099584\n",
      "Cost after iteration 12030: 0.099560\n",
      "Cost after iteration 12040: 0.099536\n",
      "Cost after iteration 12050: 0.099513\n",
      "Cost after iteration 12060: 0.099489\n",
      "Cost after iteration 12070: 0.099466\n",
      "Cost after iteration 12080: 0.099442\n",
      "Cost after iteration 12090: 0.099418\n",
      "Cost after iteration 12100: 0.099395\n",
      "Cost after iteration 12110: 0.099371\n",
      "Cost after iteration 12120: 0.099348\n",
      "Cost after iteration 12130: 0.099324\n",
      "Cost after iteration 12140: 0.099301\n",
      "Cost after iteration 12150: 0.099278\n",
      "Cost after iteration 12160: 0.099254\n",
      "Cost after iteration 12170: 0.099231\n",
      "Cost after iteration 12180: 0.099208\n",
      "Cost after iteration 12190: 0.099184\n",
      "Cost after iteration 12200: 0.099161\n",
      "Cost after iteration 12210: 0.099138\n",
      "Cost after iteration 12220: 0.099115\n",
      "Cost after iteration 12230: 0.099091\n",
      "Cost after iteration 12240: 0.099068\n",
      "Cost after iteration 12250: 0.099045\n",
      "Cost after iteration 12260: 0.099022\n",
      "Cost after iteration 12270: 0.098999\n",
      "Cost after iteration 12280: 0.098976\n",
      "Cost after iteration 12290: 0.098953\n",
      "Cost after iteration 12300: 0.098930\n",
      "Cost after iteration 12310: 0.098907\n",
      "Cost after iteration 12320: 0.098884\n",
      "Cost after iteration 12330: 0.098861\n",
      "Cost after iteration 12340: 0.098838\n",
      "Cost after iteration 12350: 0.098815\n",
      "Cost after iteration 12360: 0.098792\n",
      "Cost after iteration 12370: 0.098770\n",
      "Cost after iteration 12380: 0.098747\n",
      "Cost after iteration 12390: 0.098724\n",
      "Cost after iteration 12400: 0.098701\n",
      "Cost after iteration 12410: 0.098679\n",
      "Cost after iteration 12420: 0.098656\n",
      "Cost after iteration 12430: 0.098633\n",
      "Cost after iteration 12440: 0.098611\n",
      "Cost after iteration 12450: 0.098588\n",
      "Cost after iteration 12460: 0.098565\n",
      "Cost after iteration 12470: 0.098543\n",
      "Cost after iteration 12480: 0.098520\n",
      "Cost after iteration 12490: 0.098498\n",
      "Cost after iteration 12500: 0.098475\n",
      "Cost after iteration 12510: 0.098453\n",
      "Cost after iteration 12520: 0.098430\n",
      "Cost after iteration 12530: 0.098408\n",
      "Cost after iteration 12540: 0.098385\n",
      "Cost after iteration 12550: 0.098363\n",
      "Cost after iteration 12560: 0.098341\n",
      "Cost after iteration 12570: 0.098318\n",
      "Cost after iteration 12580: 0.098296\n",
      "Cost after iteration 12590: 0.098274\n",
      "Cost after iteration 12600: 0.098252\n",
      "Cost after iteration 12610: 0.098229\n",
      "Cost after iteration 12620: 0.098207\n",
      "Cost after iteration 12630: 0.098185\n",
      "Cost after iteration 12640: 0.098163\n",
      "Cost after iteration 12650: 0.098141\n",
      "Cost after iteration 12660: 0.098119\n",
      "Cost after iteration 12670: 0.098097\n",
      "Cost after iteration 12680: 0.098075\n",
      "Cost after iteration 12690: 0.098053\n",
      "Cost after iteration 12700: 0.098031\n",
      "Cost after iteration 12710: 0.098009\n",
      "Cost after iteration 12720: 0.097987\n",
      "Cost after iteration 12730: 0.097965\n",
      "Cost after iteration 12740: 0.097943\n",
      "Cost after iteration 12750: 0.097921\n",
      "Cost after iteration 12760: 0.097899\n",
      "Cost after iteration 12770: 0.097877\n",
      "Cost after iteration 12780: 0.097855\n",
      "Cost after iteration 12790: 0.097834\n",
      "Cost after iteration 12800: 0.097812\n",
      "Cost after iteration 12810: 0.097790\n",
      "Cost after iteration 12820: 0.097768\n",
      "Cost after iteration 12830: 0.097747\n",
      "Cost after iteration 12840: 0.097725\n",
      "Cost after iteration 12850: 0.097703\n",
      "Cost after iteration 12860: 0.097682\n",
      "Cost after iteration 12870: 0.097660\n",
      "Cost after iteration 12880: 0.097639\n",
      "Cost after iteration 12890: 0.097617\n",
      "Cost after iteration 12900: 0.097596\n",
      "Cost after iteration 12910: 0.097574\n",
      "Cost after iteration 12920: 0.097553\n",
      "Cost after iteration 12930: 0.097531\n",
      "Cost after iteration 12940: 0.097510\n",
      "Cost after iteration 12950: 0.097488\n",
      "Cost after iteration 12960: 0.097467\n",
      "Cost after iteration 12970: 0.097446\n",
      "Cost after iteration 12980: 0.097424\n",
      "Cost after iteration 12990: 0.097403\n",
      "Cost after iteration 13000: 0.097382\n",
      "Cost after iteration 13010: 0.097360\n",
      "Cost after iteration 13020: 0.097339\n",
      "Cost after iteration 13030: 0.097318\n",
      "Cost after iteration 13040: 0.097297\n",
      "Cost after iteration 13050: 0.097275\n",
      "Cost after iteration 13060: 0.097254\n",
      "Cost after iteration 13070: 0.097233\n",
      "Cost after iteration 13080: 0.097212\n",
      "Cost after iteration 13090: 0.097191\n",
      "Cost after iteration 13100: 0.097170\n",
      "Cost after iteration 13110: 0.097149\n",
      "Cost after iteration 13120: 0.097128\n",
      "Cost after iteration 13130: 0.097107\n",
      "Cost after iteration 13140: 0.097086\n",
      "Cost after iteration 13150: 0.097065\n",
      "Cost after iteration 13160: 0.097044\n",
      "Cost after iteration 13170: 0.097023\n",
      "Cost after iteration 13180: 0.097002\n",
      "Cost after iteration 13190: 0.096981\n",
      "Cost after iteration 13200: 0.096960\n",
      "Cost after iteration 13210: 0.096940\n",
      "Cost after iteration 13220: 0.096919\n",
      "Cost after iteration 13230: 0.096898\n",
      "Cost after iteration 13240: 0.096877\n",
      "Cost after iteration 13250: 0.096856\n",
      "Cost after iteration 13260: 0.096836\n",
      "Cost after iteration 13270: 0.096815\n",
      "Cost after iteration 13280: 0.096794\n",
      "Cost after iteration 13290: 0.096774\n",
      "Cost after iteration 13300: 0.096753\n",
      "Cost after iteration 13310: 0.096732\n",
      "Cost after iteration 13320: 0.096712\n",
      "Cost after iteration 13330: 0.096691\n",
      "Cost after iteration 13340: 0.096671\n",
      "Cost after iteration 13350: 0.096650\n",
      "Cost after iteration 13360: 0.096630\n",
      "Cost after iteration 13370: 0.096609\n",
      "Cost after iteration 13380: 0.096589\n",
      "Cost after iteration 13390: 0.096568\n",
      "Cost after iteration 13400: 0.096548\n",
      "Cost after iteration 13410: 0.096528\n",
      "Cost after iteration 13420: 0.096507\n",
      "Cost after iteration 13430: 0.096487\n",
      "Cost after iteration 13440: 0.096466\n",
      "Cost after iteration 13450: 0.096446\n",
      "Cost after iteration 13460: 0.096426\n",
      "Cost after iteration 13470: 0.096406\n",
      "Cost after iteration 13480: 0.096385\n",
      "Cost after iteration 13490: 0.096365\n",
      "Cost after iteration 13500: 0.096345\n",
      "Cost after iteration 13510: 0.096325\n",
      "Cost after iteration 13520: 0.096305\n",
      "Cost after iteration 13530: 0.096284\n",
      "Cost after iteration 13540: 0.096264\n",
      "Cost after iteration 13550: 0.096244\n",
      "Cost after iteration 13560: 0.096224\n",
      "Cost after iteration 13570: 0.096204\n",
      "Cost after iteration 13580: 0.096184\n",
      "Cost after iteration 13590: 0.096164\n",
      "Cost after iteration 13600: 0.096144\n",
      "Cost after iteration 13610: 0.096124\n",
      "Cost after iteration 13620: 0.096104\n",
      "Cost after iteration 13630: 0.096084\n",
      "Cost after iteration 13640: 0.096064\n",
      "Cost after iteration 13650: 0.096044\n",
      "Cost after iteration 13660: 0.096024\n",
      "Cost after iteration 13670: 0.096004\n",
      "Cost after iteration 13680: 0.095985\n",
      "Cost after iteration 13690: 0.095965\n",
      "Cost after iteration 13700: 0.095945\n",
      "Cost after iteration 13710: 0.095925\n",
      "Cost after iteration 13720: 0.095905\n",
      "Cost after iteration 13730: 0.095886\n",
      "Cost after iteration 13740: 0.095866\n",
      "Cost after iteration 13750: 0.095846\n",
      "Cost after iteration 13760: 0.095827\n",
      "Cost after iteration 13770: 0.095807\n",
      "Cost after iteration 13780: 0.095787\n",
      "Cost after iteration 13790: 0.095768\n",
      "Cost after iteration 13800: 0.095748\n",
      "Cost after iteration 13810: 0.095728\n",
      "Cost after iteration 13820: 0.095709\n",
      "Cost after iteration 13830: 0.095689\n",
      "Cost after iteration 13840: 0.095670\n",
      "Cost after iteration 13850: 0.095650\n",
      "Cost after iteration 13860: 0.095631\n",
      "Cost after iteration 13870: 0.095611\n",
      "Cost after iteration 13880: 0.095592\n",
      "Cost after iteration 13890: 0.095572\n",
      "Cost after iteration 13900: 0.095553\n",
      "Cost after iteration 13910: 0.095534\n",
      "Cost after iteration 13920: 0.095514\n",
      "Cost after iteration 13930: 0.095495\n",
      "Cost after iteration 13940: 0.095476\n",
      "Cost after iteration 13950: 0.095456\n",
      "Cost after iteration 13960: 0.095437\n",
      "Cost after iteration 13970: 0.095418\n",
      "Cost after iteration 13980: 0.095398\n",
      "Cost after iteration 13990: 0.095379\n",
      "Cost after iteration 14000: 0.095360\n",
      "Cost after iteration 14010: 0.095341\n",
      "Cost after iteration 14020: 0.095322\n",
      "Cost after iteration 14030: 0.095302\n",
      "Cost after iteration 14040: 0.095283\n",
      "Cost after iteration 14050: 0.095264\n",
      "Cost after iteration 14060: 0.095245\n",
      "Cost after iteration 14070: 0.095226\n",
      "Cost after iteration 14080: 0.095207\n",
      "Cost after iteration 14090: 0.095188\n",
      "Cost after iteration 14100: 0.095169\n",
      "Cost after iteration 14110: 0.095150\n",
      "Cost after iteration 14120: 0.095131\n",
      "Cost after iteration 14130: 0.095112\n",
      "Cost after iteration 14140: 0.095093\n",
      "Cost after iteration 14150: 0.095074\n",
      "Cost after iteration 14160: 0.095055\n",
      "Cost after iteration 14170: 0.095036\n",
      "Cost after iteration 14180: 0.095017\n",
      "Cost after iteration 14190: 0.094998\n",
      "Cost after iteration 14200: 0.094980\n",
      "Cost after iteration 14210: 0.094961\n",
      "Cost after iteration 14220: 0.094942\n",
      "Cost after iteration 14230: 0.094923\n",
      "Cost after iteration 14240: 0.094904\n",
      "Cost after iteration 14250: 0.094886\n",
      "Cost after iteration 14260: 0.094867\n",
      "Cost after iteration 14270: 0.094848\n",
      "Cost after iteration 14280: 0.094829\n",
      "Cost after iteration 14290: 0.094811\n",
      "Cost after iteration 14300: 0.094792\n",
      "Cost after iteration 14310: 0.094773\n",
      "Cost after iteration 14320: 0.094755\n",
      "Cost after iteration 14330: 0.094736\n",
      "Cost after iteration 14340: 0.094718\n",
      "Cost after iteration 14350: 0.094699\n",
      "Cost after iteration 14360: 0.094680\n",
      "Cost after iteration 14370: 0.094662\n",
      "Cost after iteration 14380: 0.094643\n",
      "Cost after iteration 14390: 0.094625\n",
      "Cost after iteration 14400: 0.094606\n",
      "Cost after iteration 14410: 0.094588\n",
      "Cost after iteration 14420: 0.094569\n",
      "Cost after iteration 14430: 0.094551\n",
      "Cost after iteration 14440: 0.094533\n",
      "Cost after iteration 14450: 0.094514\n",
      "Cost after iteration 14460: 0.094496\n",
      "Cost after iteration 14470: 0.094477\n",
      "Cost after iteration 14480: 0.094459\n",
      "Cost after iteration 14490: 0.094441\n",
      "Cost after iteration 14500: 0.094422\n",
      "Cost after iteration 14510: 0.094404\n",
      "Cost after iteration 14520: 0.094386\n",
      "Cost after iteration 14530: 0.094368\n",
      "Cost after iteration 14540: 0.094349\n",
      "Cost after iteration 14550: 0.094331\n",
      "Cost after iteration 14560: 0.094313\n",
      "Cost after iteration 14570: 0.094295\n",
      "Cost after iteration 14580: 0.094277\n",
      "Cost after iteration 14590: 0.094258\n",
      "Cost after iteration 14600: 0.094240\n",
      "Cost after iteration 14610: 0.094222\n",
      "Cost after iteration 14620: 0.094204\n",
      "Cost after iteration 14630: 0.094186\n",
      "Cost after iteration 14640: 0.094168\n",
      "Cost after iteration 14650: 0.094150\n",
      "Cost after iteration 14660: 0.094132\n",
      "Cost after iteration 14670: 0.094114\n",
      "Cost after iteration 14680: 0.094096\n",
      "Cost after iteration 14690: 0.094078\n",
      "Cost after iteration 14700: 0.094060\n",
      "Cost after iteration 14710: 0.094042\n",
      "Cost after iteration 14720: 0.094024\n",
      "Cost after iteration 14730: 0.094006\n",
      "Cost after iteration 14740: 0.093988\n",
      "Cost after iteration 14750: 0.093970\n",
      "Cost after iteration 14760: 0.093952\n",
      "Cost after iteration 14770: 0.093934\n",
      "Cost after iteration 14780: 0.093917\n",
      "Cost after iteration 14790: 0.093899\n",
      "Cost after iteration 14800: 0.093881\n",
      "Cost after iteration 14810: 0.093863\n",
      "Cost after iteration 14820: 0.093845\n",
      "Cost after iteration 14830: 0.093828\n",
      "Cost after iteration 14840: 0.093810\n",
      "Cost after iteration 14850: 0.093792\n",
      "Cost after iteration 14860: 0.093774\n",
      "Cost after iteration 14870: 0.093757\n",
      "Cost after iteration 14880: 0.093739\n",
      "Cost after iteration 14890: 0.093721\n",
      "Cost after iteration 14900: 0.093704\n",
      "Cost after iteration 14910: 0.093686\n",
      "Cost after iteration 14920: 0.093669\n",
      "Cost after iteration 14930: 0.093651\n",
      "Cost after iteration 14940: 0.093633\n",
      "Cost after iteration 14950: 0.093616\n",
      "Cost after iteration 14960: 0.093598\n",
      "Cost after iteration 14970: 0.093581\n",
      "Cost after iteration 14980: 0.093563\n",
      "Cost after iteration 14990: 0.093546\n",
      "Cost after iteration 15000: 0.093528\n",
      "Cost after iteration 15010: 0.093511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 15020: 0.093493\n",
      "Cost after iteration 15030: 0.093476\n",
      "Cost after iteration 15040: 0.093459\n",
      "Cost after iteration 15050: 0.093441\n",
      "Cost after iteration 15060: 0.093424\n",
      "Cost after iteration 15070: 0.093406\n",
      "Cost after iteration 15080: 0.093389\n",
      "Cost after iteration 15090: 0.093372\n",
      "Cost after iteration 15100: 0.093354\n",
      "Cost after iteration 15110: 0.093337\n",
      "Cost after iteration 15120: 0.093320\n",
      "Cost after iteration 15130: 0.093303\n",
      "Cost after iteration 15140: 0.093285\n",
      "Cost after iteration 15150: 0.093268\n",
      "Cost after iteration 15160: 0.093251\n",
      "Cost after iteration 15170: 0.093234\n",
      "Cost after iteration 15180: 0.093216\n",
      "Cost after iteration 15190: 0.093199\n",
      "Cost after iteration 15200: 0.093182\n",
      "Cost after iteration 15210: 0.093165\n",
      "Cost after iteration 15220: 0.093148\n",
      "Cost after iteration 15230: 0.093131\n",
      "Cost after iteration 15240: 0.093114\n",
      "Cost after iteration 15250: 0.093096\n",
      "Cost after iteration 15260: 0.093079\n",
      "Cost after iteration 15270: 0.093062\n",
      "Cost after iteration 15280: 0.093045\n",
      "Cost after iteration 15290: 0.093028\n",
      "Cost after iteration 15300: 0.093011\n",
      "Cost after iteration 15310: 0.092994\n",
      "Cost after iteration 15320: 0.092977\n",
      "Cost after iteration 15330: 0.092960\n",
      "Cost after iteration 15340: 0.092943\n",
      "Cost after iteration 15350: 0.092926\n",
      "Cost after iteration 15360: 0.092909\n",
      "Cost after iteration 15370: 0.092893\n",
      "Cost after iteration 15380: 0.092876\n",
      "Cost after iteration 15390: 0.092859\n",
      "Cost after iteration 15400: 0.092842\n",
      "Cost after iteration 15410: 0.092825\n",
      "Cost after iteration 15420: 0.092808\n",
      "Cost after iteration 15430: 0.092791\n",
      "Cost after iteration 15440: 0.092775\n",
      "Cost after iteration 15450: 0.092758\n",
      "Cost after iteration 15460: 0.092741\n",
      "Cost after iteration 15470: 0.092724\n",
      "Cost after iteration 15480: 0.092708\n",
      "Cost after iteration 15490: 0.092691\n",
      "Cost after iteration 15500: 0.092674\n",
      "Cost after iteration 15510: 0.092657\n",
      "Cost after iteration 15520: 0.092641\n",
      "Cost after iteration 15530: 0.092624\n",
      "Cost after iteration 15540: 0.092607\n",
      "Cost after iteration 15550: 0.092591\n",
      "Cost after iteration 15560: 0.092574\n",
      "Cost after iteration 15570: 0.092558\n",
      "Cost after iteration 15580: 0.092541\n",
      "Cost after iteration 15590: 0.092524\n",
      "Cost after iteration 15600: 0.092508\n",
      "Cost after iteration 15610: 0.092491\n",
      "Cost after iteration 15620: 0.092475\n",
      "Cost after iteration 15630: 0.092458\n",
      "Cost after iteration 15640: 0.092442\n",
      "Cost after iteration 15650: 0.092425\n",
      "Cost after iteration 15660: 0.092409\n",
      "Cost after iteration 15670: 0.092392\n",
      "Cost after iteration 15680: 0.092376\n",
      "Cost after iteration 15690: 0.092359\n",
      "Cost after iteration 15700: 0.092343\n",
      "Cost after iteration 15710: 0.092327\n",
      "Cost after iteration 15720: 0.092310\n",
      "Cost after iteration 15730: 0.092294\n",
      "Cost after iteration 15740: 0.092277\n",
      "Cost after iteration 15750: 0.092261\n",
      "Cost after iteration 15760: 0.092245\n",
      "Cost after iteration 15770: 0.092228\n",
      "Cost after iteration 15780: 0.092212\n",
      "Cost after iteration 15790: 0.092196\n",
      "Cost after iteration 15800: 0.092179\n",
      "Cost after iteration 15810: 0.092163\n",
      "Cost after iteration 15820: 0.092147\n",
      "Cost after iteration 15830: 0.092131\n",
      "Cost after iteration 15840: 0.092114\n",
      "Cost after iteration 15850: 0.092098\n",
      "Cost after iteration 15860: 0.092082\n",
      "Cost after iteration 15870: 0.092066\n",
      "Cost after iteration 15880: 0.092050\n",
      "Cost after iteration 15890: 0.092034\n",
      "Cost after iteration 15900: 0.092017\n",
      "Cost after iteration 15910: 0.092001\n",
      "Cost after iteration 15920: 0.091985\n",
      "Cost after iteration 15930: 0.091969\n",
      "Cost after iteration 15940: 0.091953\n",
      "Cost after iteration 15950: 0.091937\n",
      "Cost after iteration 15960: 0.091921\n",
      "Cost after iteration 15970: 0.091905\n",
      "Cost after iteration 15980: 0.091889\n",
      "Cost after iteration 15990: 0.091873\n",
      "Cost after iteration 16000: 0.091857\n",
      "Cost after iteration 16010: 0.091841\n",
      "Cost after iteration 16020: 0.091825\n",
      "Cost after iteration 16030: 0.091809\n",
      "Cost after iteration 16040: 0.091793\n",
      "Cost after iteration 16050: 0.091777\n",
      "Cost after iteration 16060: 0.091761\n",
      "Cost after iteration 16070: 0.091745\n",
      "Cost after iteration 16080: 0.091729\n",
      "Cost after iteration 16090: 0.091713\n",
      "Cost after iteration 16100: 0.091697\n",
      "Cost after iteration 16110: 0.091682\n",
      "Cost after iteration 16120: 0.091666\n",
      "Cost after iteration 16130: 0.091650\n",
      "Cost after iteration 16140: 0.091634\n",
      "Cost after iteration 16150: 0.091618\n",
      "Cost after iteration 16160: 0.091603\n",
      "Cost after iteration 16170: 0.091587\n",
      "Cost after iteration 16180: 0.091571\n",
      "Cost after iteration 16190: 0.091555\n",
      "Cost after iteration 16200: 0.091540\n",
      "Cost after iteration 16210: 0.091524\n",
      "Cost after iteration 16220: 0.091508\n",
      "Cost after iteration 16230: 0.091492\n",
      "Cost after iteration 16240: 0.091477\n",
      "Cost after iteration 16250: 0.091461\n",
      "Cost after iteration 16260: 0.091445\n",
      "Cost after iteration 16270: 0.091430\n",
      "Cost after iteration 16280: 0.091414\n",
      "Cost after iteration 16290: 0.091398\n",
      "Cost after iteration 16300: 0.091383\n",
      "Cost after iteration 16310: 0.091367\n",
      "Cost after iteration 16320: 0.091352\n",
      "Cost after iteration 16330: 0.091336\n",
      "Cost after iteration 16340: 0.091321\n",
      "Cost after iteration 16350: 0.091305\n",
      "Cost after iteration 16360: 0.091290\n",
      "Cost after iteration 16370: 0.091274\n",
      "Cost after iteration 16380: 0.091259\n",
      "Cost after iteration 16390: 0.091243\n",
      "Cost after iteration 16400: 0.091228\n",
      "Cost after iteration 16410: 0.091212\n",
      "Cost after iteration 16420: 0.091197\n",
      "Cost after iteration 16430: 0.091181\n",
      "Cost after iteration 16440: 0.091166\n",
      "Cost after iteration 16450: 0.091150\n",
      "Cost after iteration 16460: 0.091135\n",
      "Cost after iteration 16470: 0.091120\n",
      "Cost after iteration 16480: 0.091104\n",
      "Cost after iteration 16490: 0.091089\n",
      "Cost after iteration 16500: 0.091073\n",
      "Cost after iteration 16510: 0.091058\n",
      "Cost after iteration 16520: 0.091043\n",
      "Cost after iteration 16530: 0.091028\n",
      "Cost after iteration 16540: 0.091012\n",
      "Cost after iteration 16550: 0.090997\n",
      "Cost after iteration 16560: 0.090982\n",
      "Cost after iteration 16570: 0.090966\n",
      "Cost after iteration 16580: 0.090951\n",
      "Cost after iteration 16590: 0.090936\n",
      "Cost after iteration 16600: 0.090921\n",
      "Cost after iteration 16610: 0.090905\n",
      "Cost after iteration 16620: 0.090890\n",
      "Cost after iteration 16630: 0.090875\n",
      "Cost after iteration 16640: 0.090860\n",
      "Cost after iteration 16650: 0.090845\n",
      "Cost after iteration 16660: 0.090830\n",
      "Cost after iteration 16670: 0.090814\n",
      "Cost after iteration 16680: 0.090799\n",
      "Cost after iteration 16690: 0.090784\n",
      "Cost after iteration 16700: 0.090769\n",
      "Cost after iteration 16710: 0.090754\n",
      "Cost after iteration 16720: 0.090739\n",
      "Cost after iteration 16730: 0.090724\n",
      "Cost after iteration 16740: 0.090709\n",
      "Cost after iteration 16750: 0.090694\n",
      "Cost after iteration 16760: 0.090679\n",
      "Cost after iteration 16770: 0.090664\n",
      "Cost after iteration 16780: 0.090649\n",
      "Cost after iteration 16790: 0.090634\n",
      "Cost after iteration 16800: 0.090619\n",
      "Cost after iteration 16810: 0.090604\n",
      "Cost after iteration 16820: 0.090589\n",
      "Cost after iteration 16830: 0.090574\n",
      "Cost after iteration 16840: 0.090559\n",
      "Cost after iteration 16850: 0.090544\n",
      "Cost after iteration 16860: 0.090529\n",
      "Cost after iteration 16870: 0.090514\n",
      "Cost after iteration 16880: 0.090499\n",
      "Cost after iteration 16890: 0.090485\n",
      "Cost after iteration 16900: 0.090470\n",
      "Cost after iteration 16910: 0.090455\n",
      "Cost after iteration 16920: 0.090440\n",
      "Cost after iteration 16930: 0.090425\n",
      "Cost after iteration 16940: 0.090410\n",
      "Cost after iteration 16950: 0.090396\n",
      "Cost after iteration 16960: 0.090381\n",
      "Cost after iteration 16970: 0.090366\n",
      "Cost after iteration 16980: 0.090351\n",
      "Cost after iteration 16990: 0.090337\n",
      "Cost after iteration 17000: 0.090322\n",
      "Cost after iteration 17010: 0.090307\n",
      "Cost after iteration 17020: 0.090292\n",
      "Cost after iteration 17030: 0.090278\n",
      "Cost after iteration 17040: 0.090263\n",
      "Cost after iteration 17050: 0.090248\n",
      "Cost after iteration 17060: 0.090234\n",
      "Cost after iteration 17070: 0.090219\n",
      "Cost after iteration 17080: 0.090204\n",
      "Cost after iteration 17090: 0.090190\n",
      "Cost after iteration 17100: 0.090175\n",
      "Cost after iteration 17110: 0.090160\n",
      "Cost after iteration 17120: 0.090146\n",
      "Cost after iteration 17130: 0.090131\n",
      "Cost after iteration 17140: 0.090117\n",
      "Cost after iteration 17150: 0.090102\n",
      "Cost after iteration 17160: 0.090088\n",
      "Cost after iteration 17170: 0.090073\n",
      "Cost after iteration 17180: 0.090058\n",
      "Cost after iteration 17190: 0.090044\n",
      "Cost after iteration 17200: 0.090029\n",
      "Cost after iteration 17210: 0.090015\n",
      "Cost after iteration 17220: 0.090000\n",
      "Cost after iteration 17230: 0.089986\n",
      "Cost after iteration 17240: 0.089972\n",
      "Cost after iteration 17250: 0.089957\n",
      "Cost after iteration 17260: 0.089943\n",
      "Cost after iteration 17270: 0.089928\n",
      "Cost after iteration 17280: 0.089914\n",
      "Cost after iteration 17290: 0.089899\n",
      "Cost after iteration 17300: 0.089885\n",
      "Cost after iteration 17310: 0.089871\n",
      "Cost after iteration 17320: 0.089856\n",
      "Cost after iteration 17330: 0.089842\n",
      "Cost after iteration 17340: 0.089828\n",
      "Cost after iteration 17350: 0.089813\n",
      "Cost after iteration 17360: 0.089799\n",
      "Cost after iteration 17370: 0.089785\n",
      "Cost after iteration 17380: 0.089770\n",
      "Cost after iteration 17390: 0.089756\n",
      "Cost after iteration 17400: 0.089742\n",
      "Cost after iteration 17410: 0.089727\n",
      "Cost after iteration 17420: 0.089713\n",
      "Cost after iteration 17430: 0.089699\n",
      "Cost after iteration 17440: 0.089685\n",
      "Cost after iteration 17450: 0.089670\n",
      "Cost after iteration 17460: 0.089656\n",
      "Cost after iteration 17470: 0.089642\n",
      "Cost after iteration 17480: 0.089628\n",
      "Cost after iteration 17490: 0.089614\n",
      "Cost after iteration 17500: 0.089599\n",
      "Cost after iteration 17510: 0.089585\n",
      "Cost after iteration 17520: 0.089571\n",
      "Cost after iteration 17530: 0.089557\n",
      "Cost after iteration 17540: 0.089543\n",
      "Cost after iteration 17550: 0.089529\n",
      "Cost after iteration 17560: 0.089515\n",
      "Cost after iteration 17570: 0.089501\n",
      "Cost after iteration 17580: 0.089487\n",
      "Cost after iteration 17590: 0.089472\n",
      "Cost after iteration 17600: 0.089458\n",
      "Cost after iteration 17610: 0.089444\n",
      "Cost after iteration 17620: 0.089430\n",
      "Cost after iteration 17630: 0.089416\n",
      "Cost after iteration 17640: 0.089402\n",
      "Cost after iteration 17650: 0.089388\n",
      "Cost after iteration 17660: 0.089374\n",
      "Cost after iteration 17670: 0.089360\n",
      "Cost after iteration 17680: 0.089346\n",
      "Cost after iteration 17690: 0.089332\n",
      "Cost after iteration 17700: 0.089318\n",
      "Cost after iteration 17710: 0.089304\n",
      "Cost after iteration 17720: 0.089290\n",
      "Cost after iteration 17730: 0.089277\n",
      "Cost after iteration 17740: 0.089263\n",
      "Cost after iteration 17750: 0.089249\n",
      "Cost after iteration 17760: 0.089235\n",
      "Cost after iteration 17770: 0.089221\n",
      "Cost after iteration 17780: 0.089207\n",
      "Cost after iteration 17790: 0.089193\n",
      "Cost after iteration 17800: 0.089179\n",
      "Cost after iteration 17810: 0.089166\n",
      "Cost after iteration 17820: 0.089152\n",
      "Cost after iteration 17830: 0.089138\n",
      "Cost after iteration 17840: 0.089124\n",
      "Cost after iteration 17850: 0.089110\n",
      "Cost after iteration 17860: 0.089096\n",
      "Cost after iteration 17870: 0.089083\n",
      "Cost after iteration 17880: 0.089069\n",
      "Cost after iteration 17890: 0.089055\n",
      "Cost after iteration 17900: 0.089041\n",
      "Cost after iteration 17910: 0.089028\n",
      "Cost after iteration 17920: 0.089014\n",
      "Cost after iteration 17930: 0.089000\n",
      "Cost after iteration 17940: 0.088987\n",
      "Cost after iteration 17950: 0.088973\n",
      "Cost after iteration 17960: 0.088959\n",
      "Cost after iteration 17970: 0.088945\n",
      "Cost after iteration 17980: 0.088932\n",
      "Cost after iteration 17990: 0.088918\n",
      "Cost after iteration 18000: 0.088905\n",
      "Cost after iteration 18010: 0.088891\n",
      "Cost after iteration 18020: 0.088877\n",
      "Cost after iteration 18030: 0.088864\n",
      "Cost after iteration 18040: 0.088850\n",
      "Cost after iteration 18050: 0.088836\n",
      "Cost after iteration 18060: 0.088823\n",
      "Cost after iteration 18070: 0.088809\n",
      "Cost after iteration 18080: 0.088796\n",
      "Cost after iteration 18090: 0.088782\n",
      "Cost after iteration 18100: 0.088769\n",
      "Cost after iteration 18110: 0.088755\n",
      "Cost after iteration 18120: 0.088742\n",
      "Cost after iteration 18130: 0.088728\n",
      "Cost after iteration 18140: 0.088715\n",
      "Cost after iteration 18150: 0.088701\n",
      "Cost after iteration 18160: 0.088688\n",
      "Cost after iteration 18170: 0.088674\n",
      "Cost after iteration 18180: 0.088661\n",
      "Cost after iteration 18190: 0.088647\n",
      "Cost after iteration 18200: 0.088634\n",
      "Cost after iteration 18210: 0.088620\n",
      "Cost after iteration 18220: 0.088607\n",
      "Cost after iteration 18230: 0.088594\n",
      "Cost after iteration 18240: 0.088580\n",
      "Cost after iteration 18250: 0.088567\n",
      "Cost after iteration 18260: 0.088553\n",
      "Cost after iteration 18270: 0.088540\n",
      "Cost after iteration 18280: 0.088527\n",
      "Cost after iteration 18290: 0.088513\n",
      "Cost after iteration 18300: 0.088500\n",
      "Cost after iteration 18310: 0.088487\n",
      "Cost after iteration 18320: 0.088473\n",
      "Cost after iteration 18330: 0.088460\n",
      "Cost after iteration 18340: 0.088447\n",
      "Cost after iteration 18350: 0.088433\n",
      "Cost after iteration 18360: 0.088420\n",
      "Cost after iteration 18370: 0.088407\n",
      "Cost after iteration 18380: 0.088394\n",
      "Cost after iteration 18390: 0.088380\n",
      "Cost after iteration 18400: 0.088367\n",
      "Cost after iteration 18410: 0.088354\n",
      "Cost after iteration 18420: 0.088341\n",
      "Cost after iteration 18430: 0.088327\n",
      "Cost after iteration 18440: 0.088314\n",
      "Cost after iteration 18450: 0.088301\n",
      "Cost after iteration 18460: 0.088288\n",
      "Cost after iteration 18470: 0.088275\n",
      "Cost after iteration 18480: 0.088261\n",
      "Cost after iteration 18490: 0.088248\n",
      "Cost after iteration 18500: 0.088235\n",
      "Cost after iteration 18510: 0.088222\n",
      "Cost after iteration 18520: 0.088209\n",
      "Cost after iteration 18530: 0.088196\n",
      "Cost after iteration 18540: 0.088183\n",
      "Cost after iteration 18550: 0.088170\n",
      "Cost after iteration 18560: 0.088156\n",
      "Cost after iteration 18570: 0.088143\n",
      "Cost after iteration 18580: 0.088130\n",
      "Cost after iteration 18590: 0.088117\n",
      "Cost after iteration 18600: 0.088104\n",
      "Cost after iteration 18610: 0.088091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 18620: 0.088078\n",
      "Cost after iteration 18630: 0.088065\n",
      "Cost after iteration 18640: 0.088052\n",
      "Cost after iteration 18650: 0.088039\n",
      "Cost after iteration 18660: 0.088026\n",
      "Cost after iteration 18670: 0.088013\n",
      "Cost after iteration 18680: 0.088000\n",
      "Cost after iteration 18690: 0.087987\n",
      "Cost after iteration 18700: 0.087974\n",
      "Cost after iteration 18710: 0.087961\n",
      "Cost after iteration 18720: 0.087948\n",
      "Cost after iteration 18730: 0.087935\n",
      "Cost after iteration 18740: 0.087922\n",
      "Cost after iteration 18750: 0.087909\n",
      "Cost after iteration 18760: 0.087897\n",
      "Cost after iteration 18770: 0.087884\n",
      "Cost after iteration 18780: 0.087871\n",
      "Cost after iteration 18790: 0.087858\n",
      "Cost after iteration 18800: 0.087845\n",
      "Cost after iteration 18810: 0.087832\n",
      "Cost after iteration 18820: 0.087819\n",
      "Cost after iteration 18830: 0.087806\n",
      "Cost after iteration 18840: 0.087794\n",
      "Cost after iteration 18850: 0.087781\n",
      "Cost after iteration 18860: 0.087768\n",
      "Cost after iteration 18870: 0.087755\n",
      "Cost after iteration 18880: 0.087742\n",
      "Cost after iteration 18890: 0.087730\n",
      "Cost after iteration 18900: 0.087717\n",
      "Cost after iteration 18910: 0.087704\n",
      "Cost after iteration 18920: 0.087691\n",
      "Cost after iteration 18930: 0.087679\n",
      "Cost after iteration 18940: 0.087666\n",
      "Cost after iteration 18950: 0.087653\n",
      "Cost after iteration 18960: 0.087640\n",
      "Cost after iteration 18970: 0.087628\n",
      "Cost after iteration 18980: 0.087615\n",
      "Cost after iteration 18990: 0.087602\n",
      "Cost after iteration 19000: 0.087590\n",
      "Cost after iteration 19010: 0.087577\n",
      "Cost after iteration 19020: 0.087564\n",
      "Cost after iteration 19030: 0.087552\n",
      "Cost after iteration 19040: 0.087539\n",
      "Cost after iteration 19050: 0.087526\n",
      "Cost after iteration 19060: 0.087514\n",
      "Cost after iteration 19070: 0.087501\n",
      "Cost after iteration 19080: 0.087488\n",
      "Cost after iteration 19090: 0.087476\n",
      "Cost after iteration 19100: 0.087463\n",
      "Cost after iteration 19110: 0.087451\n",
      "Cost after iteration 19120: 0.087438\n",
      "Cost after iteration 19130: 0.087425\n",
      "Cost after iteration 19140: 0.087413\n",
      "Cost after iteration 19150: 0.087400\n",
      "Cost after iteration 19160: 0.087388\n",
      "Cost after iteration 19170: 0.087375\n",
      "Cost after iteration 19180: 0.087363\n",
      "Cost after iteration 19190: 0.087350\n",
      "Cost after iteration 19200: 0.087338\n",
      "Cost after iteration 19210: 0.087325\n",
      "Cost after iteration 19220: 0.087313\n",
      "Cost after iteration 19230: 0.087300\n",
      "Cost after iteration 19240: 0.087288\n",
      "Cost after iteration 19250: 0.087275\n",
      "Cost after iteration 19260: 0.087263\n",
      "Cost after iteration 19270: 0.087250\n",
      "Cost after iteration 19280: 0.087238\n",
      "Cost after iteration 19290: 0.087226\n",
      "Cost after iteration 19300: 0.087213\n",
      "Cost after iteration 19310: 0.087201\n",
      "Cost after iteration 19320: 0.087188\n",
      "Cost after iteration 19330: 0.087176\n",
      "Cost after iteration 19340: 0.087163\n",
      "Cost after iteration 19350: 0.087151\n",
      "Cost after iteration 19360: 0.087139\n",
      "Cost after iteration 19370: 0.087126\n",
      "Cost after iteration 19380: 0.087114\n",
      "Cost after iteration 19390: 0.087102\n",
      "Cost after iteration 19400: 0.087089\n",
      "Cost after iteration 19410: 0.087077\n",
      "Cost after iteration 19420: 0.087065\n",
      "Cost after iteration 19430: 0.087052\n",
      "Cost after iteration 19440: 0.087040\n",
      "Cost after iteration 19450: 0.087028\n",
      "Cost after iteration 19460: 0.087016\n",
      "Cost after iteration 19470: 0.087003\n",
      "Cost after iteration 19480: 0.086991\n",
      "Cost after iteration 19490: 0.086979\n",
      "Cost after iteration 19500: 0.086966\n",
      "Cost after iteration 19510: 0.086954\n",
      "Cost after iteration 19520: 0.086942\n",
      "Cost after iteration 19530: 0.086930\n",
      "Cost after iteration 19540: 0.086918\n",
      "Cost after iteration 19550: 0.086905\n",
      "Cost after iteration 19560: 0.086893\n",
      "Cost after iteration 19570: 0.086881\n",
      "Cost after iteration 19580: 0.086869\n",
      "Cost after iteration 19590: 0.086857\n",
      "Cost after iteration 19600: 0.086844\n",
      "Cost after iteration 19610: 0.086832\n",
      "Cost after iteration 19620: 0.086820\n",
      "Cost after iteration 19630: 0.086808\n",
      "Cost after iteration 19640: 0.086796\n",
      "Cost after iteration 19650: 0.086784\n",
      "Cost after iteration 19660: 0.086772\n",
      "Cost after iteration 19670: 0.086759\n",
      "Cost after iteration 19680: 0.086747\n",
      "Cost after iteration 19690: 0.086735\n",
      "Cost after iteration 19700: 0.086723\n",
      "Cost after iteration 19710: 0.086711\n",
      "Cost after iteration 19720: 0.086699\n",
      "Cost after iteration 19730: 0.086687\n",
      "Cost after iteration 19740: 0.086675\n",
      "Cost after iteration 19750: 0.086663\n",
      "Cost after iteration 19760: 0.086651\n",
      "Cost after iteration 19770: 0.086639\n",
      "Cost after iteration 19780: 0.086627\n",
      "Cost after iteration 19790: 0.086615\n",
      "Cost after iteration 19800: 0.086603\n",
      "Cost after iteration 19810: 0.086591\n",
      "Cost after iteration 19820: 0.086579\n",
      "Cost after iteration 19830: 0.086567\n",
      "Cost after iteration 19840: 0.086555\n",
      "Cost after iteration 19850: 0.086543\n",
      "Cost after iteration 19860: 0.086531\n",
      "Cost after iteration 19870: 0.086519\n",
      "Cost after iteration 19880: 0.086507\n",
      "Cost after iteration 19890: 0.086495\n",
      "Cost after iteration 19900: 0.086483\n",
      "Cost after iteration 19910: 0.086471\n",
      "Cost after iteration 19920: 0.086459\n",
      "Cost after iteration 19930: 0.086447\n",
      "Cost after iteration 19940: 0.086436\n",
      "Cost after iteration 19950: 0.086424\n",
      "Cost after iteration 19960: 0.086412\n",
      "Cost after iteration 19970: 0.086400\n",
      "Cost after iteration 19980: 0.086388\n",
      "Cost after iteration 19990: 0.086376\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 97.94721407624634 %\n",
      "test accuracy: 97.36842105263158 %\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[0]  # that is 4096\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    print(w)\n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "\n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "\n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.1, num_iterations = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyXOYpkrEygD",
    "outputId": "acab862f-d4a3-4df3-aeb7-1c0131cfdac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9605263157894737 \n",
      "train accuracy: 0.967741935483871 \n"
     ]
    }
   ],
   "source": [
    "# sklearn\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\n",
    "print(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\n",
    "print(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KqjHqgXFsU3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af7e9d7-2be4-42cb-a214-3a3e67a06c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "db = load_digits()\n",
    "db.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7ceb66-609b-4d8c-a5f2-37127e7e2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=8*8\n",
    "output_layers=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7fbe2e-c666-48d8-a1c4-45c7c327fec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72f74a4-9e28-4771-a1b0-c30df37a80d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8f32ec-c5ed-4138-8176-916b65173d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  9.,  0.,  2.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  8.],\n",
       "       [ 0.,  0.,  2., ...,  0.,  0.,  9.],\n",
       "       [ 0.,  0., 10., ...,  1.,  0.,  8.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=db['target'].reshape(-1,1)\n",
    "data=np.concatenate((db['data'], target), axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eeaad15-af76-40f5-8b11-2475063fc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[:,:-1]\n",
    "Y=data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3457f2f-fd0b-4acf-94de-9f617d7c2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.tensor(X, dtype=torch.float32)\n",
    "y=torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d65b320-6e22-4dbb-980a-4930a424442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff85452f-bedf-468e-82a6-390edb1ee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=RANDOM_STATE,stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5eac7b8-4d51-4068-a987-0e6038716622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 16)  \n",
    "        self.op = nn.Linear(16, 10)          \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        x = torch.softmax(self.op(x),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa7dc78-293b-40cf-a159-7edb248a4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(model, criterion, optimizer, data, targets, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26bc9be-d9d2-47d8-b13c-35a6036a893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        predicted = outputs.round().squeeze()\n",
    "        return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82af4f21-1646-4bfb-b641-5af296de05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(input_dim)\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44fe66a8-d282-4ad2-86d8-99d18f2c850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/20000], Loss: 2.2618\n",
      "Epoch [200/20000], Loss: 2.2609\n",
      "Epoch [300/20000], Loss: 2.2604\n",
      "Epoch [400/20000], Loss: 2.2606\n",
      "Epoch [500/20000], Loss: 2.2605\n",
      "Epoch [600/20000], Loss: 2.2606\n",
      "Epoch [700/20000], Loss: 2.2606\n",
      "Epoch [800/20000], Loss: 2.2606\n",
      "Epoch [900/20000], Loss: 2.2607\n",
      "Epoch [1000/20000], Loss: 2.2607\n",
      "Epoch [1100/20000], Loss: 2.2621\n",
      "Epoch [1200/20000], Loss: 2.2606\n",
      "Epoch [1300/20000], Loss: 2.0143\n",
      "Epoch [1400/20000], Loss: 1.9684\n",
      "Epoch [1500/20000], Loss: 1.9657\n",
      "Epoch [1600/20000], Loss: 1.9670\n",
      "Epoch [1700/20000], Loss: 1.9642\n",
      "Epoch [1800/20000], Loss: 1.9663\n",
      "Epoch [1900/20000], Loss: 1.9650\n",
      "Epoch [2000/20000], Loss: 1.9616\n",
      "Epoch [2100/20000], Loss: 1.8905\n",
      "Epoch [2200/20000], Loss: 1.8905\n",
      "Epoch [2300/20000], Loss: 1.8947\n",
      "Epoch [2400/20000], Loss: 1.9002\n",
      "Epoch [2500/20000], Loss: 1.8967\n",
      "Epoch [2600/20000], Loss: 1.8933\n",
      "Epoch [2700/20000], Loss: 1.9051\n",
      "Epoch [2800/20000], Loss: 1.8884\n",
      "Epoch [2900/20000], Loss: 1.8766\n",
      "Epoch [3000/20000], Loss: 1.8780\n",
      "Epoch [3100/20000], Loss: 1.8717\n",
      "Epoch [3200/20000], Loss: 1.8710\n",
      "Epoch [3300/20000], Loss: 1.8717\n",
      "Epoch [3400/20000], Loss: 1.8717\n",
      "Epoch [3500/20000], Loss: 1.8801\n",
      "Epoch [3600/20000], Loss: 1.8731\n",
      "Epoch [3700/20000], Loss: 1.8717\n",
      "Epoch [3800/20000], Loss: 1.9065\n",
      "Epoch [3900/20000], Loss: 1.8745\n",
      "Epoch [4000/20000], Loss: 1.8724\n",
      "Epoch [4100/20000], Loss: 1.8724\n",
      "Epoch [4200/20000], Loss: 1.8724\n",
      "Epoch [4300/20000], Loss: 1.8724\n",
      "Epoch [4400/20000], Loss: 1.8724\n",
      "Epoch [4500/20000], Loss: 1.8724\n",
      "Epoch [4600/20000], Loss: 1.8724\n",
      "Epoch [4700/20000], Loss: 1.8724\n",
      "Epoch [4800/20000], Loss: 1.8724\n",
      "Epoch [4900/20000], Loss: 1.8724\n",
      "Epoch [5000/20000], Loss: 1.8724\n",
      "Epoch [5100/20000], Loss: 1.8724\n",
      "Epoch [5200/20000], Loss: 1.8724\n",
      "Epoch [5300/20000], Loss: 1.8724\n",
      "Epoch [5400/20000], Loss: 1.8724\n",
      "Epoch [5500/20000], Loss: 1.8724\n",
      "Epoch [5600/20000], Loss: 1.8724\n",
      "Epoch [5700/20000], Loss: 1.8724\n",
      "Epoch [5800/20000], Loss: 1.8724\n",
      "Epoch [5900/20000], Loss: 1.8724\n",
      "Epoch [6000/20000], Loss: 1.8724\n",
      "Epoch [6100/20000], Loss: 1.8724\n",
      "Epoch [6200/20000], Loss: 1.8724\n",
      "Epoch [6300/20000], Loss: 1.8724\n",
      "Epoch [6400/20000], Loss: 1.8724\n",
      "Epoch [6500/20000], Loss: 1.8724\n",
      "Epoch [6600/20000], Loss: 1.8724\n",
      "Epoch [6700/20000], Loss: 1.8724\n",
      "Epoch [6800/20000], Loss: 1.8724\n",
      "Epoch [6900/20000], Loss: 1.8724\n",
      "Epoch [7000/20000], Loss: 1.8724\n",
      "Epoch [7100/20000], Loss: 1.8724\n",
      "Epoch [7200/20000], Loss: 1.8724\n",
      "Epoch [7300/20000], Loss: 1.8724\n",
      "Epoch [7400/20000], Loss: 1.8724\n",
      "Epoch [7500/20000], Loss: 1.8724\n",
      "Epoch [7600/20000], Loss: 1.8724\n",
      "Epoch [7700/20000], Loss: 1.8724\n",
      "Epoch [7800/20000], Loss: 1.8724\n",
      "Epoch [7900/20000], Loss: 1.8724\n",
      "Epoch [8000/20000], Loss: 1.8724\n",
      "Epoch [8100/20000], Loss: 1.8724\n",
      "Epoch [8200/20000], Loss: 1.8724\n",
      "Epoch [8300/20000], Loss: 1.8724\n",
      "Epoch [8400/20000], Loss: 1.8747\n",
      "Epoch [8500/20000], Loss: 1.8933\n",
      "Epoch [8600/20000], Loss: 1.9030\n",
      "Epoch [8700/20000], Loss: 1.8766\n",
      "Epoch [8800/20000], Loss: 1.8773\n",
      "Epoch [8900/20000], Loss: 1.8731\n",
      "Epoch [9000/20000], Loss: 1.8731\n",
      "Epoch [9100/20000], Loss: 1.8710\n",
      "Epoch [9200/20000], Loss: 1.8710\n",
      "Epoch [9300/20000], Loss: 1.8808\n",
      "Epoch [9400/20000], Loss: 1.8752\n",
      "Epoch [9500/20000], Loss: 1.8752\n",
      "Epoch [9600/20000], Loss: 1.8752\n",
      "Epoch [9700/20000], Loss: 1.8752\n",
      "Epoch [9800/20000], Loss: 1.8752\n",
      "Epoch [9900/20000], Loss: 1.8752\n",
      "Epoch [10000/20000], Loss: 1.8752\n",
      "Epoch [10100/20000], Loss: 1.8752\n",
      "Epoch [10200/20000], Loss: 1.8752\n",
      "Epoch [10300/20000], Loss: 1.8752\n",
      "Epoch [10400/20000], Loss: 1.8752\n",
      "Epoch [10500/20000], Loss: 1.8752\n",
      "Epoch [10600/20000], Loss: 1.8752\n",
      "Epoch [10700/20000], Loss: 1.8752\n",
      "Epoch [10800/20000], Loss: 1.8752\n",
      "Epoch [10900/20000], Loss: 1.8752\n",
      "Epoch [11000/20000], Loss: 1.8752\n",
      "Epoch [11100/20000], Loss: 1.8752\n",
      "Epoch [11200/20000], Loss: 1.8752\n",
      "Epoch [11300/20000], Loss: 1.8752\n",
      "Epoch [11400/20000], Loss: 1.8752\n",
      "Epoch [11500/20000], Loss: 1.8752\n",
      "Epoch [11600/20000], Loss: 1.8752\n",
      "Epoch [11700/20000], Loss: 1.8752\n",
      "Epoch [11800/20000], Loss: 1.8752\n",
      "Epoch [11900/20000], Loss: 1.8752\n",
      "Epoch [12000/20000], Loss: 1.8752\n",
      "Epoch [12100/20000], Loss: 1.8752\n",
      "Epoch [12200/20000], Loss: 1.8752\n",
      "Epoch [12300/20000], Loss: 1.8752\n",
      "Epoch [12400/20000], Loss: 1.8752\n",
      "Epoch [12500/20000], Loss: 1.8752\n",
      "Epoch [12600/20000], Loss: 1.8752\n",
      "Epoch [12700/20000], Loss: 1.8752\n",
      "Epoch [12800/20000], Loss: 1.8752\n",
      "Epoch [12900/20000], Loss: 1.8752\n",
      "Epoch [13000/20000], Loss: 1.8752\n",
      "Epoch [13100/20000], Loss: 1.8752\n",
      "Epoch [13200/20000], Loss: 1.8752\n",
      "Epoch [13300/20000], Loss: 1.8752\n",
      "Epoch [13400/20000], Loss: 1.8752\n",
      "Epoch [13500/20000], Loss: 1.8752\n",
      "Epoch [13600/20000], Loss: 1.8752\n",
      "Epoch [13700/20000], Loss: 1.8752\n",
      "Epoch [13800/20000], Loss: 1.8752\n",
      "Epoch [13900/20000], Loss: 1.8752\n",
      "Epoch [14000/20000], Loss: 1.8752\n",
      "Epoch [14100/20000], Loss: 1.8752\n",
      "Epoch [14200/20000], Loss: 1.8752\n",
      "Epoch [14300/20000], Loss: 1.8752\n",
      "Epoch [14400/20000], Loss: 1.8752\n",
      "Epoch [14500/20000], Loss: 1.8752\n",
      "Epoch [14600/20000], Loss: 1.8752\n",
      "Epoch [14700/20000], Loss: 1.8752\n",
      "Epoch [14800/20000], Loss: 1.8752\n",
      "Epoch [14900/20000], Loss: 1.8752\n",
      "Epoch [15000/20000], Loss: 1.8752\n",
      "Epoch [15100/20000], Loss: 1.8752\n",
      "Epoch [15200/20000], Loss: 1.8752\n",
      "Epoch [15300/20000], Loss: 1.8752\n",
      "Epoch [15400/20000], Loss: 1.8752\n",
      "Epoch [15500/20000], Loss: 1.8752\n",
      "Epoch [15600/20000], Loss: 1.8752\n",
      "Epoch [15700/20000], Loss: 1.8752\n",
      "Epoch [15800/20000], Loss: 1.8752\n",
      "Epoch [15900/20000], Loss: 1.8752\n",
      "Epoch [16000/20000], Loss: 1.8752\n",
      "Epoch [16100/20000], Loss: 1.8752\n",
      "Epoch [16200/20000], Loss: 1.8752\n",
      "Epoch [16300/20000], Loss: 1.8752\n",
      "Epoch [16400/20000], Loss: 1.8752\n",
      "Epoch [16500/20000], Loss: 1.8752\n",
      "Epoch [16600/20000], Loss: 1.8752\n",
      "Epoch [16700/20000], Loss: 1.8752\n",
      "Epoch [16800/20000], Loss: 1.8740\n",
      "Epoch [16900/20000], Loss: 1.8740\n",
      "Epoch [17000/20000], Loss: 1.8740\n",
      "Epoch [17100/20000], Loss: 1.8740\n",
      "Epoch [17200/20000], Loss: 1.8740\n",
      "Epoch [17300/20000], Loss: 1.8740\n",
      "Epoch [17400/20000], Loss: 1.8740\n",
      "Epoch [17500/20000], Loss: 1.8740\n",
      "Epoch [17600/20000], Loss: 1.8740\n",
      "Epoch [17700/20000], Loss: 1.8740\n",
      "Epoch [17800/20000], Loss: 1.8740\n",
      "Epoch [17900/20000], Loss: 1.8740\n",
      "Epoch [18000/20000], Loss: 1.8740\n",
      "Epoch [18100/20000], Loss: 1.8740\n",
      "Epoch [18200/20000], Loss: 1.8740\n",
      "Epoch [18300/20000], Loss: 1.8740\n",
      "Epoch [18400/20000], Loss: 1.8740\n",
      "Epoch [18500/20000], Loss: 1.8740\n",
      "Epoch [18600/20000], Loss: 1.8740\n",
      "Epoch [18700/20000], Loss: 1.8740\n",
      "Epoch [18800/20000], Loss: 1.8740\n",
      "Epoch [18900/20000], Loss: 1.8740\n",
      "Epoch [19000/20000], Loss: 1.8740\n",
      "Epoch [19100/20000], Loss: 1.8740\n",
      "Epoch [19200/20000], Loss: 1.8740\n",
      "Epoch [19300/20000], Loss: 1.8740\n",
      "Epoch [19400/20000], Loss: 1.8740\n",
      "Epoch [19500/20000], Loss: 1.8740\n",
      "Epoch [19600/20000], Loss: 1.8740\n",
      "Epoch [19700/20000], Loss: 1.8740\n",
      "Epoch [19800/20000], Loss: 1.8740\n",
      "Epoch [19900/20000], Loss: 1.8740\n",
      "Epoch [20000/20000], Loss: 1.8740\n"
     ]
    }
   ],
   "source": [
    "train_mlp(mlp, criterion, optimizer, x_train,y_train, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97edf7d-b96c-4053-8dc1-a4e6bff3ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d3fdd-0f51-4a01-ab50-ab4154409571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d4b1e-dc4d-44d7-baa3-344b0e27195f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

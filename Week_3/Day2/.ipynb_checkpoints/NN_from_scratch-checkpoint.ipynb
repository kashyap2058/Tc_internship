{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3472bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85097fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "          0.01990749, -0.01764613],\n",
       "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "         -0.06833155, -0.09220405],\n",
       "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "          0.00286131, -0.02593034],\n",
       "        ...,\n",
       "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "         -0.04688253,  0.01549073],\n",
       "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "          0.04452873, -0.02593034],\n",
       "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "         -0.00422151,  0.00306441]]),\n",
       " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "        220.,  57.]),\n",
       " 'frame': None,\n",
       " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, total serum cholesterol\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, total cholesterol / HDL\\n      - s5      ltg, possibly log of serum triglycerides level\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
       " 'feature_names': ['age',\n",
       "  'sex',\n",
       "  'bmi',\n",
       "  'bp',\n",
       "  's1',\n",
       "  's2',\n",
       "  's3',\n",
       "  's4',\n",
       "  's5',\n",
       "  's6'],\n",
       " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
       " 'target_filename': 'diabetes_target.csv.gz',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data=load_diabetes()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb28a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[151.],\n",
       "       [ 75.],\n",
       "       [141.],\n",
       "       [206.],\n",
       "       [135.],\n",
       "       [ 97.],\n",
       "       [138.],\n",
       "       [ 63.],\n",
       "       [110.],\n",
       "       [310.],\n",
       "       [101.],\n",
       "       [ 69.],\n",
       "       [179.],\n",
       "       [185.],\n",
       "       [118.],\n",
       "       [171.],\n",
       "       [166.],\n",
       "       [144.],\n",
       "       [ 97.],\n",
       "       [168.],\n",
       "       [ 68.],\n",
       "       [ 49.],\n",
       "       [ 68.],\n",
       "       [245.],\n",
       "       [184.],\n",
       "       [202.],\n",
       "       [137.],\n",
       "       [ 85.],\n",
       "       [131.],\n",
       "       [283.],\n",
       "       [129.],\n",
       "       [ 59.],\n",
       "       [341.],\n",
       "       [ 87.],\n",
       "       [ 65.],\n",
       "       [102.],\n",
       "       [265.],\n",
       "       [276.],\n",
       "       [252.],\n",
       "       [ 90.],\n",
       "       [100.],\n",
       "       [ 55.],\n",
       "       [ 61.],\n",
       "       [ 92.],\n",
       "       [259.],\n",
       "       [ 53.],\n",
       "       [190.],\n",
       "       [142.],\n",
       "       [ 75.],\n",
       "       [142.],\n",
       "       [155.],\n",
       "       [225.],\n",
       "       [ 59.],\n",
       "       [104.],\n",
       "       [182.],\n",
       "       [128.],\n",
       "       [ 52.],\n",
       "       [ 37.],\n",
       "       [170.],\n",
       "       [170.],\n",
       "       [ 61.],\n",
       "       [144.],\n",
       "       [ 52.],\n",
       "       [128.],\n",
       "       [ 71.],\n",
       "       [163.],\n",
       "       [150.],\n",
       "       [ 97.],\n",
       "       [160.],\n",
       "       [178.],\n",
       "       [ 48.],\n",
       "       [270.],\n",
       "       [202.],\n",
       "       [111.],\n",
       "       [ 85.],\n",
       "       [ 42.],\n",
       "       [170.],\n",
       "       [200.],\n",
       "       [252.],\n",
       "       [113.],\n",
       "       [143.],\n",
       "       [ 51.],\n",
       "       [ 52.],\n",
       "       [210.],\n",
       "       [ 65.],\n",
       "       [141.],\n",
       "       [ 55.],\n",
       "       [134.],\n",
       "       [ 42.],\n",
       "       [111.],\n",
       "       [ 98.],\n",
       "       [164.],\n",
       "       [ 48.],\n",
       "       [ 96.],\n",
       "       [ 90.],\n",
       "       [162.],\n",
       "       [150.],\n",
       "       [279.],\n",
       "       [ 92.],\n",
       "       [ 83.],\n",
       "       [128.],\n",
       "       [102.],\n",
       "       [302.],\n",
       "       [198.],\n",
       "       [ 95.],\n",
       "       [ 53.],\n",
       "       [134.],\n",
       "       [144.],\n",
       "       [232.],\n",
       "       [ 81.],\n",
       "       [104.],\n",
       "       [ 59.],\n",
       "       [246.],\n",
       "       [297.],\n",
       "       [258.],\n",
       "       [229.],\n",
       "       [275.],\n",
       "       [281.],\n",
       "       [179.],\n",
       "       [200.],\n",
       "       [200.],\n",
       "       [173.],\n",
       "       [180.],\n",
       "       [ 84.],\n",
       "       [121.],\n",
       "       [161.],\n",
       "       [ 99.],\n",
       "       [109.],\n",
       "       [115.],\n",
       "       [268.],\n",
       "       [274.],\n",
       "       [158.],\n",
       "       [107.],\n",
       "       [ 83.],\n",
       "       [103.],\n",
       "       [272.],\n",
       "       [ 85.],\n",
       "       [280.],\n",
       "       [336.],\n",
       "       [281.],\n",
       "       [118.],\n",
       "       [317.],\n",
       "       [235.],\n",
       "       [ 60.],\n",
       "       [174.],\n",
       "       [259.],\n",
       "       [178.],\n",
       "       [128.],\n",
       "       [ 96.],\n",
       "       [126.],\n",
       "       [288.],\n",
       "       [ 88.],\n",
       "       [292.],\n",
       "       [ 71.],\n",
       "       [197.],\n",
       "       [186.],\n",
       "       [ 25.],\n",
       "       [ 84.],\n",
       "       [ 96.],\n",
       "       [195.],\n",
       "       [ 53.],\n",
       "       [217.],\n",
       "       [172.],\n",
       "       [131.],\n",
       "       [214.],\n",
       "       [ 59.],\n",
       "       [ 70.],\n",
       "       [220.],\n",
       "       [268.],\n",
       "       [152.],\n",
       "       [ 47.],\n",
       "       [ 74.],\n",
       "       [295.],\n",
       "       [101.],\n",
       "       [151.],\n",
       "       [127.],\n",
       "       [237.],\n",
       "       [225.],\n",
       "       [ 81.],\n",
       "       [151.],\n",
       "       [107.],\n",
       "       [ 64.],\n",
       "       [138.],\n",
       "       [185.],\n",
       "       [265.],\n",
       "       [101.],\n",
       "       [137.],\n",
       "       [143.],\n",
       "       [141.],\n",
       "       [ 79.],\n",
       "       [292.],\n",
       "       [178.],\n",
       "       [ 91.],\n",
       "       [116.],\n",
       "       [ 86.],\n",
       "       [122.],\n",
       "       [ 72.],\n",
       "       [129.],\n",
       "       [142.],\n",
       "       [ 90.],\n",
       "       [158.],\n",
       "       [ 39.],\n",
       "       [196.],\n",
       "       [222.],\n",
       "       [277.],\n",
       "       [ 99.],\n",
       "       [196.],\n",
       "       [202.],\n",
       "       [155.],\n",
       "       [ 77.],\n",
       "       [191.],\n",
       "       [ 70.],\n",
       "       [ 73.],\n",
       "       [ 49.],\n",
       "       [ 65.],\n",
       "       [263.],\n",
       "       [248.],\n",
       "       [296.],\n",
       "       [214.],\n",
       "       [185.],\n",
       "       [ 78.],\n",
       "       [ 93.],\n",
       "       [252.],\n",
       "       [150.],\n",
       "       [ 77.],\n",
       "       [208.],\n",
       "       [ 77.],\n",
       "       [108.],\n",
       "       [160.],\n",
       "       [ 53.],\n",
       "       [220.],\n",
       "       [154.],\n",
       "       [259.],\n",
       "       [ 90.],\n",
       "       [246.],\n",
       "       [124.],\n",
       "       [ 67.],\n",
       "       [ 72.],\n",
       "       [257.],\n",
       "       [262.],\n",
       "       [275.],\n",
       "       [177.],\n",
       "       [ 71.],\n",
       "       [ 47.],\n",
       "       [187.],\n",
       "       [125.],\n",
       "       [ 78.],\n",
       "       [ 51.],\n",
       "       [258.],\n",
       "       [215.],\n",
       "       [303.],\n",
       "       [243.],\n",
       "       [ 91.],\n",
       "       [150.],\n",
       "       [310.],\n",
       "       [153.],\n",
       "       [346.],\n",
       "       [ 63.],\n",
       "       [ 89.],\n",
       "       [ 50.],\n",
       "       [ 39.],\n",
       "       [103.],\n",
       "       [308.],\n",
       "       [116.],\n",
       "       [145.],\n",
       "       [ 74.],\n",
       "       [ 45.],\n",
       "       [115.],\n",
       "       [264.],\n",
       "       [ 87.],\n",
       "       [202.],\n",
       "       [127.],\n",
       "       [182.],\n",
       "       [241.],\n",
       "       [ 66.],\n",
       "       [ 94.],\n",
       "       [283.],\n",
       "       [ 64.],\n",
       "       [102.],\n",
       "       [200.],\n",
       "       [265.],\n",
       "       [ 94.],\n",
       "       [230.],\n",
       "       [181.],\n",
       "       [156.],\n",
       "       [233.],\n",
       "       [ 60.],\n",
       "       [219.],\n",
       "       [ 80.],\n",
       "       [ 68.],\n",
       "       [332.],\n",
       "       [248.],\n",
       "       [ 84.],\n",
       "       [200.],\n",
       "       [ 55.],\n",
       "       [ 85.],\n",
       "       [ 89.],\n",
       "       [ 31.],\n",
       "       [129.],\n",
       "       [ 83.],\n",
       "       [275.],\n",
       "       [ 65.],\n",
       "       [198.],\n",
       "       [236.],\n",
       "       [253.],\n",
       "       [124.],\n",
       "       [ 44.],\n",
       "       [172.],\n",
       "       [114.],\n",
       "       [142.],\n",
       "       [109.],\n",
       "       [180.],\n",
       "       [144.],\n",
       "       [163.],\n",
       "       [147.],\n",
       "       [ 97.],\n",
       "       [220.],\n",
       "       [190.],\n",
       "       [109.],\n",
       "       [191.],\n",
       "       [122.],\n",
       "       [230.],\n",
       "       [242.],\n",
       "       [248.],\n",
       "       [249.],\n",
       "       [192.],\n",
       "       [131.],\n",
       "       [237.],\n",
       "       [ 78.],\n",
       "       [135.],\n",
       "       [244.],\n",
       "       [199.],\n",
       "       [270.],\n",
       "       [164.],\n",
       "       [ 72.],\n",
       "       [ 96.],\n",
       "       [306.],\n",
       "       [ 91.],\n",
       "       [214.],\n",
       "       [ 95.],\n",
       "       [216.],\n",
       "       [263.],\n",
       "       [178.],\n",
       "       [113.],\n",
       "       [200.],\n",
       "       [139.],\n",
       "       [139.],\n",
       "       [ 88.],\n",
       "       [148.],\n",
       "       [ 88.],\n",
       "       [243.],\n",
       "       [ 71.],\n",
       "       [ 77.],\n",
       "       [109.],\n",
       "       [272.],\n",
       "       [ 60.],\n",
       "       [ 54.],\n",
       "       [221.],\n",
       "       [ 90.],\n",
       "       [311.],\n",
       "       [281.],\n",
       "       [182.],\n",
       "       [321.],\n",
       "       [ 58.],\n",
       "       [262.],\n",
       "       [206.],\n",
       "       [233.],\n",
       "       [242.],\n",
       "       [123.],\n",
       "       [167.],\n",
       "       [ 63.],\n",
       "       [197.],\n",
       "       [ 71.],\n",
       "       [168.],\n",
       "       [140.],\n",
       "       [217.],\n",
       "       [121.],\n",
       "       [235.],\n",
       "       [245.],\n",
       "       [ 40.],\n",
       "       [ 52.],\n",
       "       [104.],\n",
       "       [132.],\n",
       "       [ 88.],\n",
       "       [ 69.],\n",
       "       [219.],\n",
       "       [ 72.],\n",
       "       [201.],\n",
       "       [110.],\n",
       "       [ 51.],\n",
       "       [277.],\n",
       "       [ 63.],\n",
       "       [118.],\n",
       "       [ 69.],\n",
       "       [273.],\n",
       "       [258.],\n",
       "       [ 43.],\n",
       "       [198.],\n",
       "       [242.],\n",
       "       [232.],\n",
       "       [175.],\n",
       "       [ 93.],\n",
       "       [168.],\n",
       "       [275.],\n",
       "       [293.],\n",
       "       [281.],\n",
       "       [ 72.],\n",
       "       [140.],\n",
       "       [189.],\n",
       "       [181.],\n",
       "       [209.],\n",
       "       [136.],\n",
       "       [261.],\n",
       "       [113.],\n",
       "       [131.],\n",
       "       [174.],\n",
       "       [257.],\n",
       "       [ 55.],\n",
       "       [ 84.],\n",
       "       [ 42.],\n",
       "       [146.],\n",
       "       [212.],\n",
       "       [233.],\n",
       "       [ 91.],\n",
       "       [111.],\n",
       "       [152.],\n",
       "       [120.],\n",
       "       [ 67.],\n",
       "       [310.],\n",
       "       [ 94.],\n",
       "       [183.],\n",
       "       [ 66.],\n",
       "       [173.],\n",
       "       [ 72.],\n",
       "       [ 49.],\n",
       "       [ 64.],\n",
       "       [ 48.],\n",
       "       [178.],\n",
       "       [104.],\n",
       "       [132.],\n",
       "       [220.],\n",
       "       [ 57.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=data['target'].reshape(-1,1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f4265c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03807591,  0.05068012,  0.06169621,  0.02187239, -0.0442235 ,\n",
       "       -0.03482076, -0.04340085, -0.00259226,  0.01990749, -0.01764613])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771a4dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.13348416289594"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a95fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c16861f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.where(target >= 140, 1, 0)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8666ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data = np.concatenate((data['data'], target), axis=1)\n",
    "combined_data[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3653b859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f38c97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "          0.01990749, -0.01764613],\n",
       "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "         -0.06833155, -0.09220405],\n",
       "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "          0.00286131, -0.02593034],\n",
       "        ...,\n",
       "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "         -0.04688253,  0.01549073],\n",
       "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "          0.04452873, -0.02593034],\n",
       "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "         -0.00422151,  0.00306441]]),\n",
       " array([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the data is preprocessed and converted into categorical data, let us design a neural network\n",
    "#before this lets split the data\n",
    "x=combined_data[:,:-1]\n",
    "y=combined_data[:,-1]\n",
    "# y=y.reshape(-1,1)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a062c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8b51c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fcd754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0]),len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "271fc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19697836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    hidden_layer1 = np.zeros((5,1))\n",
    "    weights1 = np.random.rand(5,(len(x[0])))-0.5\n",
    "#     print(weights1)\n",
    "    hidden_layer2 = np.zeros((3,1))\n",
    "    weights2 = np.random.rand(3, 5)-0.5\n",
    "    output_layer = np.zeros((1,1))\n",
    "#     print(output_layer)\n",
    "    op_weights = np.random.rand(1, 3)-0.5\n",
    "    return weights1,weights2,op_weights,hidden_layer1,hidden_layer2,output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5039df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d29bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f29e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48395392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(x):\n",
    "    return relu(x)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f591b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss\n",
    "def bceloss(output, actual):\n",
    "#     print(\"actual\",actual.shape)\n",
    "#     print(\"predicted\",output.shape)\n",
    "    value = -np.mean(actual * np.log(output) + (1 - actual) * np.log(1 - output))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3988ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(w1,w2,w3,b1,b2,b3,x):\n",
    "    z1=np.dot(w1,x.T)+b1\n",
    "    a1=relu(z1)\n",
    "    z2=np.dot(w2,a1)+b2\n",
    "    a2=relu(z2)\n",
    "    z3=np.dot(w3,a2)+b3\n",
    "    a3=relu(z3)\n",
    "#     print(z1.shape)\n",
    "#     print(z2.shape)\n",
    "#     print(z3.shape)\n",
    "    return z1,z2,z3,a1,a2,a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dc1158a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,z1,z2,z3,a1,a2,a3,w1,w2,w3):\n",
    "    de3=(a3-y)*derivative_sigmoid(z3)\n",
    "    db3=de3\n",
    "    dw3=np.dot(de3,a2.T)\n",
    "    da2=np.dot(w3.T,de3)\n",
    "#     print('Da2',da2.shape)\n",
    "#     print('Dw3 ',dw3.shape)\n",
    "#     print(w3.shape)\n",
    "#     print(de3.shape)\n",
    "    de2=da2*derivative_relu(z2)\n",
    "    db2=de2\n",
    "    dw2=np.dot(de2,a1.T)\n",
    "    da1=np.dot(w2.T,de2)\n",
    "#     print('Da1',da1.shape)\n",
    "#     print('Dw2 ',dw2.shape)\n",
    "#     print(w2.shape)\n",
    "#     print(de2.shape)\n",
    "#     print(z1.shape)\n",
    "\n",
    "    de1=da1*derivative_relu(z1)\n",
    "    db1=de1\n",
    "    dw1=np.dot(de1,x)\n",
    "#     print('Dw1',dw1.shape)\n",
    "    return dw1,dw2,dw3,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3245772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1,w2,w3,b1,b2,b3=init()\n",
    "# z1,z2,z3,a1,a2,a3=forward_prop(w1,w2,w3,b1,b2,b3,x)\n",
    "# gradient(x,y,z1,z2,z3,a1,a2,a3,w1,w2,w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7ade7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWeightsAndBiases(w1,w2,w3,b1,b2,b3,dw1,dw2,dw3,db1,db2,db3,alpha):\n",
    "    w1=w1-alpha*dw1\n",
    "    b1=b1-alpha*db1\n",
    "    w2=w2-alpha*dw2\n",
    "    b2=b2-alpha*db2\n",
    "    w3=w3-alpha*dw3\n",
    "    b3=b3-alpha*db3\n",
    "    return w1,w2,w3,b1,b2,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8084772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w1, w2, w3, b1, b2, b3, x):\n",
    "    _, _, _, _, _, a3 = forward_prop(w1, w2, w3, b1, b2, b3, x)\n",
    "    predictions = (a3 > 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c613050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, actual):\n",
    "    predictions = (output > 0.5).astype(int)\n",
    "    correct_predictions = np.mean(predictions == actual)\n",
    "    return correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "93ff13e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 2/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 3/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 4/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 5/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 6/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 7/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 8/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 9/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 10/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 11/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 12/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 13/50, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 14/50, Cost: nan, Accuracy: 0.49838187702265374\n",
      "Epoch 15/50, Cost: nan, Accuracy: 0.5501618122977346\n",
      "Epoch 16/50, Cost: nan, Accuracy: 0.7508090614886731\n",
      "Epoch 17/50, Cost: nan, Accuracy: 0.9061488673139159\n",
      "Epoch 18/50, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 19/50, Cost: nan, Accuracy: 0.9773462783171522\n",
      "Epoch 20/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 21/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 22/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 23/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 24/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 25/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 26/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 27/50, Cost: nan, Accuracy: 0.9838187702265372\n",
      "Epoch 28/50, Cost: nan, Accuracy: 0.9870550161812298\n",
      "Epoch 29/50, Cost: nan, Accuracy: 0.9870550161812298\n",
      "Epoch 30/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 31/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 32/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 33/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 34/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 35/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 36/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 37/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 38/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 39/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 40/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 41/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 42/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 43/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 44/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 45/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 46/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 47/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 48/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 49/50, Cost: nan, Accuracy: 1.0\n",
      "Epoch 50/50, Cost: nan, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kashyap Ghimire\\AppData\\Local\\Temp\\ipykernel_20456\\248879514.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  value = -np.mean(actual * np.log(output) + (1 - actual) * np.log(1 - output))\n",
      "C:\\Users\\Kashyap Ghimire\\AppData\\Local\\Temp\\ipykernel_20456\\248879514.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  value = -np.mean(actual * np.log(output) + (1 - actual) * np.log(1 - output))\n",
      "C:\\Users\\Kashyap Ghimire\\AppData\\Local\\Temp\\ipykernel_20456\\248879514.py:5: RuntimeWarning: invalid value encountered in log\n",
      "  value = -np.mean(actual * np.log(output) + (1 - actual) * np.log(1 - output))\n"
     ]
    }
   ],
   "source": [
    "alpha=0.1\n",
    "epoch=50\n",
    "w1,w2,w3,b1,b2,b3=init()\n",
    "for i in range(epoch):\n",
    "    z1,z2,z3,a1,a2,a3=forward_prop(w1,w2,w3,b1,b2,b3,x_train)\n",
    "    dw1,dw2,dw3,db1,db2,db3=gradient(x_train,y_train,z1,z2,z3,a1,a2,a3,w1,w2,w3)\n",
    "    w1,w2,w3,b1,b2,b3=updateWeightsAndBiases(w1,w2,w3,b1,b2,b3,dw1,dw2,dw3,db1,db2,db3,alpha)\n",
    "    cost=bceloss(output=a3,actual=y_train)\n",
    "    train_accuracy = accuracy(a3, y_train)\n",
    "    print(f'Epoch {i+1}/{epoch}, Cost: {cost}, Accuracy: {train_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d5b0a",
   "metadata": {},
   "source": [
    "The value of BCE loss is nan because of some values that are equal to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73ba34",
   "metadata": {},
   "source": [
    "To overcome this let us implement a different cost function suitable for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4bb3a419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00006578e+00, 1.00020646e+00, 0.00000000e+00, 9.99860577e-01,\n",
       "        9.99602942e-01, 1.85947425e-04, 2.66097181e-04, 9.99887077e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.98382447e-04, 1.00019743e+00,\n",
       "        1.00005317e+00, 2.29076002e-04, 1.93568656e-05, 3.64012205e-04,\n",
       "        1.00020603e+00, 0.00000000e+00, 7.73567686e-04, 1.00027239e+00,\n",
       "        1.00019929e+00, 1.09416356e-05, 0.00000000e+00, 1.00009699e+00,\n",
       "        1.00015453e+00, 1.00026405e+00, 1.22977343e-04, 1.00010137e+00,\n",
       "        1.00014553e+00, 1.00006767e+00, 4.18851603e-04, 1.00009686e+00,\n",
       "        1.00008653e+00, 1.00009148e+00, 7.09947768e-06, 6.92109592e-01,\n",
       "        5.45341004e-04, 1.00005434e+00, 1.00022504e+00, 1.00009301e+00,\n",
       "        1.00002951e+00, 3.44644010e-04, 8.49765980e-05, 1.00009209e+00,\n",
       "        1.56220064e-03, 0.00000000e+00, 9.99995724e-01, 1.00014422e+00,\n",
       "        1.84201587e-04, 1.00014325e+00, 1.00004990e+00, 9.99912450e-01,\n",
       "        4.24155878e-05, 0.00000000e+00, 9.12390897e-04, 5.26667216e-04,\n",
       "        9.99364305e-01, 1.00023811e+00, 1.66257656e-04, 1.00029681e+00,\n",
       "        1.00008931e+00, 1.00025754e+00, 0.00000000e+00, 1.70742510e-04,\n",
       "        4.53544290e-06, 1.00005347e+00, 0.00000000e+00, 1.00012635e+00,\n",
       "        1.00006108e+00, 1.00012754e+00, 9.99787255e-01, 7.20667004e-04,\n",
       "        5.74559397e-05, 4.51640127e-04, 1.00010190e+00, 4.31702971e-05,\n",
       "        1.00004827e+00, 1.00007724e+00, 5.43148296e-05, 1.00005724e+00,\n",
       "        0.00000000e+00, 9.99894251e-01, 0.00000000e+00, 1.00017478e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00003798e+00, 9.99872685e-01,\n",
       "        1.00003902e+00, 2.76399538e-04, 1.00002337e+00, 3.10162672e-04,\n",
       "        1.15930792e-04, 9.99859372e-01, 1.00005960e+00, 1.00018086e+00,\n",
       "        3.82426397e-04, 1.00002949e+00, 9.99967444e-01, 4.07820211e-04,\n",
       "        4.07857926e-04, 9.99758632e-01, 1.73707916e-05, 1.00006075e+00,\n",
       "        3.28748774e-04, 0.00000000e+00, 0.00000000e+00, 8.52354099e-04,\n",
       "        3.75934341e-04, 0.00000000e+00, 2.83213539e-05, 1.00007722e+00,\n",
       "        0.00000000e+00, 5.38804872e-05, 1.85064544e-05, 0.00000000e+00,\n",
       "        1.00007749e+00, 1.00013312e+00, 1.00014258e+00, 1.58021476e-04,\n",
       "        1.00010100e+00, 1.00026289e+00, 1.00010294e+00, 1.00011301e+00,\n",
       "        0.00000000e+00, 1.00008267e+00, 3.13795427e-05, 1.00005460e+00,\n",
       "        1.00022255e+00, 1.00012659e+00, 1.08781913e-05, 1.00006138e+00,\n",
       "        8.50755484e-04, 7.82682767e-05, 0.00000000e+00, 9.99853884e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.94286812e-05, 3.78454727e-04,\n",
       "        4.08373761e-07, 1.00025921e+00, 6.45584787e-05, 9.99914245e-01,\n",
       "        1.00007029e+00, 1.00025097e+00, 0.00000000e+00, 1.02552885e-05,\n",
       "        0.00000000e+00, 9.99954801e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.16439335e-04, 1.00021343e+00, 1.00005175e+00, 1.00011246e+00,\n",
       "        0.00000000e+00, 1.00003145e+00, 1.00012522e+00, 1.28999979e-04,\n",
       "        6.56302960e-05, 4.05639418e-04, 1.00007625e+00, 1.00004721e+00,\n",
       "        1.00009927e+00, 0.00000000e+00, 9.92196671e-01, 1.00031350e+00,\n",
       "        0.00000000e+00, 2.56486277e-04, 5.59608046e-05, 9.99934709e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00001072e+00, 9.96875592e-01,\n",
       "        1.50568654e-04, 4.40632360e-04, 1.00007263e+00, 3.26447186e-04,\n",
       "        1.00012943e+00, 9.99986956e-01, 1.00020221e+00, 1.00015105e+00,\n",
       "        1.00018229e+00, 2.27012883e-06, 2.03896841e-05, 0.00000000e+00,\n",
       "        4.73094557e-05, 1.00013222e+00, 1.00013794e+00, 0.00000000e+00,\n",
       "        1.00012200e+00, 0.00000000e+00, 5.74810728e-04, 1.00002440e+00,\n",
       "        1.26867325e-05, 9.99737469e-01, 9.82194008e-01, 1.00005431e+00,\n",
       "        1.45432312e-04, 0.00000000e+00, 1.00018542e+00, 2.42115333e-04,\n",
       "        7.50529612e-04, 6.92109592e-01, 1.00009130e+00, 1.00019030e+00,\n",
       "        1.00013041e+00, 1.00012336e+00, 6.92109592e-01, 6.92109592e-01,\n",
       "        9.99595576e-01, 1.09977698e-04, 5.72429069e-04, 0.00000000e+00,\n",
       "        4.90326030e-04, 1.00006288e+00, 9.99973448e-01, 5.36826798e-04,\n",
       "        0.00000000e+00, 9.93159934e-01, 1.00000183e+00, 1.11905031e-05,\n",
       "        1.00006291e+00, 6.09331673e-04, 3.45542227e-06, 4.90537030e-04,\n",
       "        1.00021424e+00, 0.00000000e+00, 1.00024961e+00, 4.57616405e-04,\n",
       "        1.00002797e+00, 9.99975903e-01, 4.21018147e-04, 0.00000000e+00,\n",
       "        1.00004654e+00, 1.00013269e+00, 1.00014145e+00, 8.93497350e-05,\n",
       "        1.82350818e-04, 0.00000000e+00, 6.55097547e-05, 6.61390741e-06,\n",
       "        2.30686782e-04, 1.00004723e+00, 0.00000000e+00, 1.00011082e+00,\n",
       "        0.00000000e+00, 1.00016220e+00, 0.00000000e+00, 1.00005639e+00,\n",
       "        1.00025031e+00, 1.00003132e+00, 3.14854699e-04, 2.11240055e-04,\n",
       "        4.35170309e-04, 1.00016035e+00, 3.64043163e-07, 1.00015657e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00009717e+00, 9.99886794e-01,\n",
       "        1.00006313e+00, 7.38060041e-05, 1.00016502e+00, 1.00003668e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00001710e+00, 0.00000000e+00,\n",
       "        1.00009857e+00, 0.00000000e+00, 4.65989368e-04, 0.00000000e+00,\n",
       "        1.00022457e+00, 0.00000000e+00, 3.46978058e-05, 4.85961542e-06,\n",
       "        1.00015461e+00, 4.23499883e-04, 1.00009074e+00, 1.00007244e+00,\n",
       "        4.36227100e-04, 1.67713869e-04, 1.00006247e+00, 1.00016563e+00,\n",
       "        3.87117427e-04, 9.99947383e-01, 1.00012231e+00, 1.00019372e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.99967099e-01, 3.48344910e-04,\n",
       "        3.74374614e-04, 1.00014971e+00, 9.99980097e-01, 1.00029895e+00,\n",
       "        4.55899027e-06, 1.31207478e-05, 1.00031092e+00, 1.71069919e-05,\n",
       "        1.00012246e+00, 1.56778012e-05, 1.00016547e+00, 1.00007133e+00,\n",
       "        1.00001246e+00]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "87b34f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.zeros((a3.size))\n",
    "i=0\n",
    "for item in a3[0]:\n",
    "    if item>0.5:\n",
    "        b[i]=1\n",
    "    else:\n",
    "        b[i]=0\n",
    "    i+=1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "87995050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to predicr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e24e608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8427faca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bc5fa",
   "metadata": {},
   "source": [
    "predicting values from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0f1e5ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,133) (5,309) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred)\n",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(w1, w2, w3, b1, b2, b3, x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(w1, w2, w3, b1, b2, b3, x):\n\u001b[1;32m----> 2\u001b[0m     _, _, _, _, _, a3 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m (a3 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m, in \u001b[0;36mforward_prop\u001b[1;34m(w1, w2, w3, b1, b2, b3, x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_prop\u001b[39m(w1,w2,w3,b1,b2,b3,x):\n\u001b[1;32m----> 2\u001b[0m     z1\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mb1\u001b[49m\n\u001b[0;32m      3\u001b[0m     a1\u001b[38;5;241m=\u001b[39mrelu(z1)\n\u001b[0;32m      4\u001b[0m     z2\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(w2,a1)\u001b[38;5;241m+\u001b[39mb2\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,133) (5,309) "
     ]
    }
   ],
   "source": [
    "# y_pred = predict(w1, w2, w3, b1, b2, b3, x_test)\n",
    "# print(\"Predicted labels:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "95bd9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to fix this we add 0 padding to the test set\n",
    "def change_shape(x_test):\n",
    "    desired_shape = (309, 10)\n",
    "\n",
    "    # Calculate padding\n",
    "    pad_width = [(0, desired_shape[0] - x_test.shape[0]), (0, 0)]\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    new_x_test = np.pad(x_test, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    print(\"Original shape:\", x_test.shape)\n",
    "    print(\"Padded shape:\", new_x_test.shape)\n",
    "    return new_x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed461684",
   "metadata": {},
   "source": [
    "The above given code throws an error because the matrix used to train the network has different dimensions compared to the one that is used to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5c7026d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (133, 10)\n",
      "Padded shape: (309, 10)\n",
      "Predicted labels: [[1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
      "  0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
      "  0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
      "  1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0\n",
      "  1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0\n",
      "  0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
      "  1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1\n",
      "  0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "new_x_test=change_shape(x_test)\n",
    "y_pred = predict(w1, w2, w3, b1, b2, b3, new_x_test)\n",
    "print(\"Predicted labels:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1777c071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actal labels are:  [0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.]\n",
      "The predicted labels are:  [[1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
      "  0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
      "  0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#however the predicted values are top 133 values of the network\n",
    "print(\"The actal labels are: \", y_test)\n",
    "print(\"The predicted labels are: \",y_pred[0:,:133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a4224ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hence the model has overfit\n",
    "#to check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b319f774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 2/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 3/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 4/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 5/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 6/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 7/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 8/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 9/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 10/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 11/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 12/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 13/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 14/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 15/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 16/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 17/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 18/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 19/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 20/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 21/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 22/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 23/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 24/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 25/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 26/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 27/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 28/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 29/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 30/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 31/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 32/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 33/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 34/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 35/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 36/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 37/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 38/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 39/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 40/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 41/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 42/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 43/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 44/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 45/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 46/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 47/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 48/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 49/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 50/145, Cost: nan, Accuracy: 0.49514563106796117\n",
      "Epoch 51/145, Cost: nan, Accuracy: 0.49838187702265374\n",
      "Epoch 52/145, Cost: nan, Accuracy: 0.49838187702265374\n",
      "Epoch 53/145, Cost: nan, Accuracy: 0.5016181229773463\n",
      "Epoch 54/145, Cost: nan, Accuracy: 0.5080906148867314\n",
      "Epoch 55/145, Cost: nan, Accuracy: 0.5242718446601942\n",
      "Epoch 56/145, Cost: nan, Accuracy: 0.5436893203883495\n",
      "Epoch 57/145, Cost: nan, Accuracy: 0.5501618122977346\n",
      "Epoch 58/145, Cost: nan, Accuracy: 0.56957928802589\n",
      "Epoch 59/145, Cost: nan, Accuracy: 0.5857605177993528\n",
      "Epoch 60/145, Cost: nan, Accuracy: 0.6084142394822006\n",
      "Epoch 61/145, Cost: nan, Accuracy: 0.6213592233009708\n",
      "Epoch 62/145, Cost: nan, Accuracy: 0.6343042071197411\n",
      "Epoch 63/145, Cost: nan, Accuracy: 0.6407766990291263\n",
      "Epoch 64/145, Cost: nan, Accuracy: 0.6472491909385113\n",
      "Epoch 65/145, Cost: nan, Accuracy: 0.6666666666666666\n",
      "Epoch 66/145, Cost: nan, Accuracy: 0.6893203883495146\n",
      "Epoch 67/145, Cost: nan, Accuracy: 0.7119741100323624\n",
      "Epoch 68/145, Cost: nan, Accuracy: 0.7411003236245954\n",
      "Epoch 69/145, Cost: nan, Accuracy: 0.7475728155339806\n",
      "Epoch 70/145, Cost: nan, Accuracy: 0.7605177993527508\n",
      "Epoch 71/145, Cost: nan, Accuracy: 0.7669902912621359\n",
      "Epoch 72/145, Cost: nan, Accuracy: 0.7702265372168284\n",
      "Epoch 73/145, Cost: nan, Accuracy: 0.7766990291262136\n",
      "Epoch 74/145, Cost: nan, Accuracy: 0.7928802588996764\n",
      "Epoch 75/145, Cost: nan, Accuracy: 0.8090614886731392\n",
      "Epoch 76/145, Cost: nan, Accuracy: 0.8220064724919094\n",
      "Epoch 77/145, Cost: nan, Accuracy: 0.8252427184466019\n",
      "Epoch 78/145, Cost: nan, Accuracy: 0.8381877022653722\n",
      "Epoch 79/145, Cost: nan, Accuracy: 0.8414239482200647\n",
      "Epoch 80/145, Cost: nan, Accuracy: 0.8478964401294499\n",
      "Epoch 81/145, Cost: nan, Accuracy: 0.8576051779935275\n",
      "Epoch 82/145, Cost: nan, Accuracy: 0.8673139158576052\n",
      "Epoch 83/145, Cost: nan, Accuracy: 0.8737864077669902\n",
      "Epoch 84/145, Cost: nan, Accuracy: 0.8802588996763754\n",
      "Epoch 85/145, Cost: nan, Accuracy: 0.8996763754045307\n",
      "Epoch 86/145, Cost: nan, Accuracy: 0.9061488673139159\n",
      "Epoch 87/145, Cost: nan, Accuracy: 0.912621359223301\n",
      "Epoch 88/145, Cost: nan, Accuracy: 0.9158576051779935\n",
      "Epoch 89/145, Cost: nan, Accuracy: 0.9158576051779935\n",
      "Epoch 90/145, Cost: nan, Accuracy: 0.9158576051779935\n",
      "Epoch 91/145, Cost: nan, Accuracy: 0.9223300970873787\n",
      "Epoch 92/145, Cost: nan, Accuracy: 0.9223300970873787\n",
      "Epoch 93/145, Cost: nan, Accuracy: 0.9288025889967637\n",
      "Epoch 94/145, Cost: nan, Accuracy: 0.9320388349514563\n",
      "Epoch 95/145, Cost: nan, Accuracy: 0.9320388349514563\n",
      "Epoch 96/145, Cost: nan, Accuracy: 0.9320388349514563\n",
      "Epoch 97/145, Cost: nan, Accuracy: 0.9320388349514563\n",
      "Epoch 98/145, Cost: nan, Accuracy: 0.9320388349514563\n",
      "Epoch 99/145, Cost: nan, Accuracy: 0.9320388349514563\n",
      "Epoch 100/145, Cost: nan, Accuracy: 0.9352750809061489\n",
      "Epoch 101/145, Cost: nan, Accuracy: 0.9352750809061489\n",
      "Epoch 102/145, Cost: nan, Accuracy: 0.9385113268608414\n",
      "Epoch 103/145, Cost: nan, Accuracy: 0.941747572815534\n",
      "Epoch 104/145, Cost: nan, Accuracy: 0.9449838187702265\n",
      "Epoch 105/145, Cost: nan, Accuracy: 0.9449838187702265\n",
      "Epoch 106/145, Cost: nan, Accuracy: 0.9449838187702265\n",
      "Epoch 107/145, Cost: nan, Accuracy: 0.9449838187702265\n",
      "Epoch 108/145, Cost: nan, Accuracy: 0.9449838187702265\n",
      "Epoch 109/145, Cost: nan, Accuracy: 0.948220064724919\n",
      "Epoch 110/145, Cost: nan, Accuracy: 0.9514563106796117\n",
      "Epoch 111/145, Cost: nan, Accuracy: 0.9546925566343042\n",
      "Epoch 112/145, Cost: nan, Accuracy: 0.9546925566343042\n",
      "Epoch 113/145, Cost: nan, Accuracy: 0.9579288025889967\n",
      "Epoch 114/145, Cost: nan, Accuracy: 0.9579288025889967\n",
      "Epoch 115/145, Cost: nan, Accuracy: 0.9579288025889967\n",
      "Epoch 116/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 117/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 118/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 119/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 120/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 121/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 122/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 123/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 124/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 125/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 126/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 127/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 128/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 129/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 130/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 131/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 132/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 133/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 134/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 135/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 136/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 137/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 138/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 139/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 140/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 141/145, Cost: nan, Accuracy: 0.9611650485436893\n",
      "Epoch 142/145, Cost: nan, Accuracy: 0.9644012944983819\n",
      "Epoch 143/145, Cost: nan, Accuracy: 0.9644012944983819\n",
      "Epoch 144/145, Cost: nan, Accuracy: 0.9644012944983819\n",
      "Epoch 145/145, Cost: nan, Accuracy: 0.9644012944983819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kashyap Ghimire\\AppData\\Local\\Temp\\ipykernel_20456\\248879514.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  value = -np.mean(actual * np.log(output) + (1 - actual) * np.log(1 - output))\n",
      "C:\\Users\\Kashyap Ghimire\\AppData\\Local\\Temp\\ipykernel_20456\\248879514.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  value = -np.mean(actual * np.log(output) + (1 - actual) * np.log(1 - output))\n",
      "C:\\Users\\Kashyap Ghimire\\AppData\\Local\\Temp\\ipykernel_20456\\248879514.py:5: RuntimeWarning: invalid value encountered in log\n",
      "  value = -np.mean(actual * np.log(output) + (1 - actual) * np.log(1 - output))\n"
     ]
    }
   ],
   "source": [
    "alpha=0.012\n",
    "epoch=145\n",
    "w1,w2,w3,b1,b2,b3=init()\n",
    "for i in range(epoch):\n",
    "    z1,z2,z3,a1,a2,a3=forward_prop(w1,w2,w3,b1,b2,b3,x_train)\n",
    "    dw1,dw2,dw3,db1,db2,db3=gradient(x_train,y_train,z1,z2,z3,a1,a2,a3,w1,w2,w3)\n",
    "    w1,w2,w3,b1,b2,b3=updateWeightsAndBiases(w1,w2,w3,b1,b2,b3,dw1,dw2,dw3,db1,db2,db3,alpha)\n",
    "    cost=bceloss(output=a3,actual=y_train)\n",
    "    train_accuracy = accuracy(a3, y_train)\n",
    "    print(f'Epoch {i+1}/{epoch}, Cost: {cost}, Accuracy: {train_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00928571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (133, 10)\n",
      "Padded shape: (309, 10)\n",
      "The actal labels are:  [0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.]\n",
      "The predicted labels are:  [[1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1\n",
      "  0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0\n",
      "  0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#the model got 96 percent accuracy\n",
    "new_x_test=change_shape(x_test)\n",
    "y_pred = predict(w1, w2, w3, b1, b2, b3, new_x_test)\n",
    "print(\"The actal labels are: \", y_test)\n",
    "print(\"The predicted labels are: \",y_pred[0:,:133])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81d5d9",
   "metadata": {},
   "source": [
    "hence The model performs poorly on test data but performs good on train data(the model is overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "56acf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('weights.npz', w1=w1, w2=w2,w3=w3, b1=b1,b2=b2,b3=b3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
